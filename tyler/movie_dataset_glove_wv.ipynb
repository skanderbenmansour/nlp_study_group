{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "source is https://www.kaggle.com/c/word2vec-nlp-tutorial/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/tyler/Documents/programming/pytorch_nlp/data/word2vec-nlp-tutorial/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path+'labeledTrainData.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[:15000]\n",
    "val = df[15000:20000]\n",
    "test = df[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 5000, 5000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train),len(val),len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_review(review):\n",
    "    chars = ['/','\\\\','>','<','-','br']\n",
    "    chars.extend('1 2 3 4 5 6 7 8 9 0'.split())\n",
    "    for char in chars:\n",
    "        review = review.replace(char,'')\n",
    "    \n",
    "    tokens = word_tokenize(review)\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make vocab and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e619e4290a3d4dcd92c68b339d7d9a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labels = list(train.sentiment)\n",
    "reviews = list(train.review.values)\n",
    "\n",
    "all_words = [process_review(review) for review in tqdm(reviews)]\n",
    "\n",
    "train_data = list(zip(all_words,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in all_words for item in sublist]\n",
    "vocab = set(flat_list)\n",
    "\n",
    "len(vocab)\n",
    "\n",
    "word_to_idx = {word:idx for idx,word in enumerate(list(vocab))}\n",
    "\n",
    "counts = Counter(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(flat_list).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 10\n",
    "keep = counts[start:20000+start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 52046), ('this', 45732), ('that', 44178), (\"'s\", 37794), ('was', 30368)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [word for word,count in keep]\n",
    "vocab.append('UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {word:idx for idx,word in enumerate(list(vocab))}\n",
    "idx_to_word = {idx:word for word,idx in word_to_idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a04e05f4c784f49a02031ba86b845fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labels = list(test.sentiment)\n",
    "reviews = list(test.review.values)\n",
    "\n",
    "all_words = [process_review(review) for review in tqdm(reviews)]\n",
    "\n",
    "test_data = list(zip(all_words,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9bcf46ba5845828d06b233b101317e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labels = list(val.sentiment)\n",
    "reviews = list(val.review.values)\n",
    "\n",
    "all_words = [process_review(review) for review in tqdm(reviews)]\n",
    "\n",
    "val_data = list(zip(all_words,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pretained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/tyler/Documents/programming/embeddings/models/glove.840B.300d.model'\n",
    "glove = KeyedVectors.load_word2vec_format(path,limit=100000)\n",
    "weights = torch.FloatTensor(glove.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.8155)"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([0,1],dtype=torch.float)\n",
    "b = torch.tensor([.5,.5],dtype=torch.float)\n",
    "loss_function(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100000, 300])"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of'"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.index2word[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word:idx for idx,word in enumerate(glove.vocab.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input(sentence, word2idx):\n",
    "    vec = torch.zeros(len(word2idx),dtype=torch.long)\n",
    "    for word in sentence:\n",
    "        if word in word2idx:\n",
    "            vec[word2idx[word]] = 1\n",
    "    return vec.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_padded_input(sentence, word2idx):\n",
    "    vec = np.zeros((50,300))\n",
    "    vec_idx = 0\n",
    "    for word in sentence:\n",
    "        if vec_idx < 50:\n",
    "            if word in glove:\n",
    "                vec[vec_idx] = glove[word]\n",
    "                vec_idx += 1\n",
    "                \n",
    "    vec = vec.mean(axis=0)\n",
    "    ten = torch.tensor(vec,dtype=torch.float)\n",
    "    \n",
    "    return ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ', , this is a sentence asdf'.split()\n",
    "x = make_padded_input(sentence, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class glove_classifier(nn.Module):\n",
    "    def __init__(self, num_labels, vocab_size, embedding_dim, hidden,glove,max_len):\n",
    "        super(glove_classifier, self).__init__()\n",
    "        self.linear = nn.Linear(embedding_dim * max_len, hidden)\n",
    "        self.linear_2 = nn.Linear(hidden, num_labels)\n",
    "        self.dropout = nn.Dropout(p=.3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs.view(1,-1)\n",
    "        x = self.linear(x)\n",
    "        #rint(x.shape)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        #print(x.shape)\n",
    "        return F.softmax((x), dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = 2\n",
    "VOCAB_SIZE = 100000\n",
    "embedding_dim = 300\n",
    "hidden = 100\n",
    "max_len = 1\n",
    "\n",
    "model = glove_classifier(NUM_LABELS, VOCAB_SIZE,embedding_dim,hidden,glove,max_len)\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "lr = .001\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ', , this is a sentence asdf'.split()\n",
    "x = make_padded_input(sentence, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5207, 0.4793], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: train loss of 0.601, val loss of 0.557\n",
      "Val loss decreased inf --> 0.557 saving model...\n",
      "Epoch 2/20: train loss of 0.544, val loss of 0.550\n",
      "Val loss decreased 0.557 --> 0.550 saving model...\n",
      "Epoch 3/20: train loss of 0.535, val loss of 0.543\n",
      "Val loss decreased 0.550 --> 0.543 saving model...\n",
      "Epoch 4/20: train loss of 0.530, val loss of 0.540\n",
      "Val loss decreased 0.543 --> 0.540 saving model...\n",
      "Epoch 5/20: train loss of 0.526, val loss of 0.538\n",
      "Val loss decreased 0.540 --> 0.538 saving model...\n",
      "Epoch 6/20: train loss of 0.523, val loss of 0.538\n",
      "Val loss decreased 0.538 --> 0.538 saving model...\n",
      "Epoch 7/20: train loss of 0.520, val loss of 0.537\n",
      "Val loss decreased 0.538 --> 0.537 saving model...\n",
      "Epoch 8/20: train loss of 0.517, val loss of 0.535\n",
      "Val loss decreased 0.537 --> 0.535 saving model...\n",
      "Epoch 9/20: train loss of 0.515, val loss of 0.533\n",
      "Val loss decreased 0.535 --> 0.533 saving model...\n",
      "Epoch 10/20: train loss of 0.512, val loss of 0.532\n",
      "Val loss decreased 0.533 --> 0.532 saving model...\n",
      "Epoch 11/20: train loss of 0.510, val loss of 0.530\n",
      "Val loss decreased 0.532 --> 0.530 saving model...\n",
      "Epoch 12/20: train loss of 0.508, val loss of 0.529\n",
      "Val loss decreased 0.530 --> 0.529 saving model...\n",
      "Epoch 13/20: train loss of 0.506, val loss of 0.528\n",
      "Val loss decreased 0.529 --> 0.528 saving model...\n",
      "Epoch 14/20: train loss of 0.505, val loss of 0.527\n",
      "Val loss decreased 0.528 --> 0.527 saving model...\n",
      "Epoch 15/20: train loss of 0.503, val loss of 0.527\n",
      "Val loss decreased 0.527 --> 0.527 saving model...\n",
      "Epoch 16/20: train loss of 0.501, val loss of 0.527\n",
      "Val loss decreased 0.527 --> 0.527 saving model...\n",
      "Epoch 17/20: train loss of 0.500, val loss of 0.527\n",
      "Epoch 18/20: train loss of 0.499, val loss of 0.527\n",
      "Epoch 19/20: train loss of 0.497, val loss of 0.528\n",
      "Epoch 20/20: train loss of 0.496, val loss of 0.528\n"
     ]
    }
   ],
   "source": [
    "save_path = 'model_checkpoints/glove_classifier.pt'\n",
    "val_loss_min = np.Inf\n",
    "#val_loss_min = 0.694\n",
    "num_epochs = 20\n",
    "loss_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for sentence, label in train_data:\n",
    "        model.zero_grad()\n",
    "\n",
    "        vec = make_padded_input(sentence, word2idx)\n",
    "        if label == 0:\n",
    "            target = torch.tensor([1,0],dtype=torch.float)\n",
    "        else:\n",
    "            target = torch.tensor([0,1],dtype=torch.float)\n",
    "        #target = torch.LongTensor([label])\n",
    "\n",
    "        log_probs = model(vec)\n",
    "\n",
    "        loss = loss_function(log_probs, target)\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    mean_train_loss = np.mean(train_loss)\n",
    "    val_loss = []\n",
    "    for sentence, label in val_data[:5000]:\n",
    "        model.eval()\n",
    "\n",
    "        vec = make_padded_input(sentence, word2idx)\n",
    "        if label == 0:\n",
    "            target = torch.tensor([1,0],dtype=torch.float)\n",
    "        else:\n",
    "            target = torch.tensor([0,1],dtype=torch.float)\n",
    "        #target = torch.LongTensor([label])\n",
    "\n",
    "        log_probs = model(vec)\n",
    "        pred = log_probs.argmax().detach().numpy()\n",
    "        loss = loss_function(log_probs, target)\n",
    "\n",
    "        val_loss.append(loss.item())\n",
    "        \n",
    "    mean_val_loss = np.mean(val_loss)\n",
    "    \n",
    "    loss_history.append((mean_train_loss,mean_val_loss))\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}: train loss of {mean_train_loss:.3f}, val loss of {mean_val_loss:.3f}')\n",
    "    \n",
    "    if mean_val_loss <= val_loss_min:\n",
    "        print(f'Val loss decreased {val_loss_min:.3f} --> {mean_val_loss:.3f} saving model...')\n",
    "        torch.save(model.state_dict(),save_path)\n",
    "        val_loss_min = mean_val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----TRAIN SET----\n",
      "train loss of 0.522\n",
      "train accuracy of 74.12\n",
      "----VAL SET----\n",
      "val loss of 0.528\n",
      "val accuracy of 73.72\n",
      "----TEST SET----\n",
      "test loss of 0.554\n",
      "test accuracy of 71.5\n"
     ]
    }
   ],
   "source": [
    "names = 'train val test'.split()\n",
    "num = 100\n",
    "#data_list = [train_data[:num],val_data[:num],test_data[:num]]\n",
    "data_list = [train_data,val_data,test_data]\n",
    "\n",
    "for name,data in zip(names,data_list):\n",
    "    eval_loss = []\n",
    "    num_correct = 0\n",
    "    to_eval = test_data\n",
    "    for sentence, label in data:\n",
    "        model.eval()\n",
    "\n",
    "        vec = make_padded_input(sentence, word2idx)        \n",
    "        if label == 0:\n",
    "            target = torch.tensor([1,0],dtype=torch.float)\n",
    "        else:\n",
    "            target = torch.tensor([0,1],dtype=torch.float)\n",
    "\n",
    "        log_probs = model(vec)\n",
    "        pred = log_probs.argmax().detach().numpy()\n",
    "        correct = int(pred == label)\n",
    "        num_correct += correct\n",
    "        loss = loss_function(log_probs, target)\n",
    "\n",
    "        eval_loss.append(loss.item())\n",
    "    \n",
    "    mean_loss = np.mean(eval_loss)\n",
    "    print(f'----{name} set----'.upper())\n",
    "    print(f'{name} loss of {round(mean_loss,3)}')\n",
    "    print(f'{name} accuracy of {round(num_correct*100/len(data),2)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
