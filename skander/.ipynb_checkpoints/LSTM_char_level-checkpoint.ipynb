{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2859,
     "status": "ok",
     "timestamp": 1588318405493,
     "user": {
      "displayName": "Skander Ben Mansour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggl7domjq2nPtW4SYKR5BEYuj4qkl2Ljge9t-vlBiU=s64",
      "userId": "11506237701449758837"
     },
     "user_tz": -60
    },
    "id": "P8X5BSymDQ_w",
    "outputId": "35b9aa74-f020-4981-cf2b-473768c2815c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x27c88d43c10>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1036,
     "status": "ok",
     "timestamp": 1588318405497,
     "user": {
      "displayName": "Skander Ben Mansour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggl7domjq2nPtW4SYKR5BEYuj4qkl2Ljge9t-vlBiU=s64",
      "userId": "11506237701449758837"
     },
     "user_tz": -60
    },
    "id": "64VoojxzDQ_5",
    "outputId": "6550080b-ab81-4b64-a577-e58ae2724b90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 1.4697, -0.3951, -0.5101,  1.1163, -0.5926,  0.9089, -1.0792, -0.6751,\n",
      "          1.4083, -1.8456]]), tensor([[-0.5655, -0.9094, -0.4891, -0.6175,  0.3836,  0.3852,  0.7890,  1.1363,\n",
      "         -0.1147, -0.0180]]), tensor([[ 0.0491,  0.4272, -0.8996,  0.5313,  0.4034,  1.4521, -2.4182, -1.1906,\n",
      "          0.6964,  1.1296]]), tensor([[ 0.2214, -0.0558,  1.2057,  1.9486, -0.0766, -0.8562, -0.7870, -0.8161,\n",
      "          0.5470, -1.1707]]), tensor([[-0.4699, -1.6271, -0.1127,  1.5980, -0.8445, -1.0489,  0.9387,  0.5378,\n",
      "          1.5372, -0.6943]])]\n",
      "tensor([[[ 1.4697, -0.3951, -0.5101,  1.1163, -0.5926,  0.9089, -1.0792,\n",
      "          -0.6751,  1.4083, -1.8456]],\n",
      "\n",
      "        [[-0.5655, -0.9094, -0.4891, -0.6175,  0.3836,  0.3852,  0.7890,\n",
      "           1.1363, -0.1147, -0.0180]],\n",
      "\n",
      "        [[ 0.0491,  0.4272, -0.8996,  0.5313,  0.4034,  1.4521, -2.4182,\n",
      "          -1.1906,  0.6964,  1.1296]],\n",
      "\n",
      "        [[ 0.2214, -0.0558,  1.2057,  1.9486, -0.0766, -0.8562, -0.7870,\n",
      "          -0.8161,  0.5470, -1.1707]],\n",
      "\n",
      "        [[-0.4699, -1.6271, -0.1127,  1.5980, -0.8445, -1.0489,  0.9387,\n",
      "           0.5378,  1.5372, -0.6943]]])\n",
      "tensor([[[ 0.5595, -0.0880, -0.1437, -0.4697,  0.2218]],\n",
      "\n",
      "        [[ 0.0775, -0.0024, -0.3386, -0.2280,  0.3244]],\n",
      "\n",
      "        [[-0.0792, -0.1593, -0.0167, -0.3246,  0.0927]],\n",
      "\n",
      "        [[ 0.1874, -0.2819, -0.0086, -0.0823,  0.0315]],\n",
      "\n",
      "        [[ 0.2854, -0.2004, -0.0710, -0.1411,  0.3178]]],\n",
      "       grad_fn=<CatBackward>)\n",
      "(tensor([[[ 0.2854, -0.2004, -0.0710, -0.1411,  0.3178]]], grad_fn=<ViewBackward>), tensor([[[ 0.7698, -0.6869, -0.3282, -0.3147,  0.3889]]], grad_fn=<ViewBackward>))\n"
     ]
    }
   ],
   "source": [
    "# SENTENCE LSTM EXAMPLE\n",
    "\n",
    "\n",
    "# We imagine we have a  word embedding space of space of dim 10 and the LSTM returns an embedding of dim 5\n",
    "lstm = nn.LSTM(10, 5) \n",
    "inputs = [torch.randn(1, 10) for _ in range(5)]  # make a sentence of length 5\n",
    "print(inputs)\n",
    "\n",
    "# initialize the hidden state.\n",
    "hidden = (torch.randn(1, 1, 5),\n",
    "          torch.randn(1, 1, 5))\n",
    "\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "\n",
    "## or all at once\n",
    "\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "print(inputs)\n",
    "# clean out hidden state like above\n",
    "hidden = (torch.randn(1, 1, 5),\n",
    "          torch.randn(1, 1, 5))  \n",
    "\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 857
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 492,
     "status": "ok",
     "timestamp": 1588318476253,
     "user": {
      "displayName": "Skander Ben Mansour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggl7domjq2nPtW4SYKR5BEYuj4qkl2Ljge9t-vlBiU=s64",
      "userId": "11506237701449758837"
     },
     "user_tz": -60
    },
    "id": "3yrgMmhqSnKv",
    "outputId": "a3feb924-a7d4-4c93-cd8d-4bfcd6e57f43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-1.5116,  0.1907,  0.2044,  0.1639, -1.2945, -0.1286, -0.0571, -0.0711,\n",
      "          1.1658,  0.1701, -1.0919,  0.0826,  0.0131, -1.1464, -0.2603, -0.3115,\n",
      "          0.8936, -1.0561, -0.5676,  0.4355,  0.0008, -0.0789,  0.8751,  0.5548,\n",
      "         -0.9748, -0.5950]]), tensor([[ 0.8703,  0.8653,  0.2820,  0.9745,  0.1880, -0.9599, -0.4766, -2.0444,\n",
      "          1.3598, -0.3804,  1.5289, -1.6573,  0.0821,  1.1764,  0.0877, -0.9174,\n",
      "          0.9862,  0.4376,  0.0470, -0.0343, -0.0269, -0.3772,  0.2464,  0.5727,\n",
      "          0.5574,  0.3598]]), tensor([[ 0.9314, -2.4962,  0.7922,  1.1106,  0.1389,  1.1380,  0.5049, -1.1455,\n",
      "         -0.1846, -1.4921,  0.4998, -0.8199,  0.4210, -1.7431, -1.6584,  1.7252,\n",
      "          1.3536,  1.6513,  0.0011, -0.8320,  0.6357, -0.7775, -0.0557, -0.0443,\n",
      "          2.0859,  0.1051]]), tensor([[ 0.1401, -1.0772,  1.1557, -1.0234,  0.9199,  1.3019,  0.9390,  0.8462,\n",
      "          0.9443, -0.7599,  0.6024,  0.1352, -0.6365,  0.0678,  1.0910, -0.1847,\n",
      "          0.0786, -0.8521,  1.1760,  0.9729,  0.0173,  0.7971, -0.2596,  0.3957,\n",
      "          0.4421,  0.9448]]), tensor([[ 0.3672,  1.5841, -2.3037,  0.1520,  0.1131, -1.4705, -0.5454,  0.0985,\n",
      "          0.2416,  0.5700, -1.4720,  1.2683,  0.7452, -0.3816, -1.8310, -0.5801,\n",
      "         -0.4347,  0.7958, -0.3111, -1.0572, -1.6718,  1.2436,  1.4090, -1.7305,\n",
      "         -1.0663, -0.1203]])]\n",
      "tensor([[[-1.5116,  0.1907,  0.2044,  0.1639, -1.2945, -0.1286, -0.0571,\n",
      "          -0.0711,  1.1658,  0.1701, -1.0919,  0.0826,  0.0131, -1.1464,\n",
      "          -0.2603, -0.3115,  0.8936, -1.0561, -0.5676,  0.4355,  0.0008,\n",
      "          -0.0789,  0.8751,  0.5548, -0.9748, -0.5950]],\n",
      "\n",
      "        [[ 0.8703,  0.8653,  0.2820,  0.9745,  0.1880, -0.9599, -0.4766,\n",
      "          -2.0444,  1.3598, -0.3804,  1.5289, -1.6573,  0.0821,  1.1764,\n",
      "           0.0877, -0.9174,  0.9862,  0.4376,  0.0470, -0.0343, -0.0269,\n",
      "          -0.3772,  0.2464,  0.5727,  0.5574,  0.3598]],\n",
      "\n",
      "        [[ 0.9314, -2.4962,  0.7922,  1.1106,  0.1389,  1.1380,  0.5049,\n",
      "          -1.1455, -0.1846, -1.4921,  0.4998, -0.8199,  0.4210, -1.7431,\n",
      "          -1.6584,  1.7252,  1.3536,  1.6513,  0.0011, -0.8320,  0.6357,\n",
      "          -0.7775, -0.0557, -0.0443,  2.0859,  0.1051]],\n",
      "\n",
      "        [[ 0.1401, -1.0772,  1.1557, -1.0234,  0.9199,  1.3019,  0.9390,\n",
      "           0.8462,  0.9443, -0.7599,  0.6024,  0.1352, -0.6365,  0.0678,\n",
      "           1.0910, -0.1847,  0.0786, -0.8521,  1.1760,  0.9729,  0.0173,\n",
      "           0.7971, -0.2596,  0.3957,  0.4421,  0.9448]],\n",
      "\n",
      "        [[ 0.3672,  1.5841, -2.3037,  0.1520,  0.1131, -1.4705, -0.5454,\n",
      "           0.0985,  0.2416,  0.5700, -1.4720,  1.2683,  0.7452, -0.3816,\n",
      "          -1.8310, -0.5801, -0.4347,  0.7958, -0.3111, -1.0572, -1.6718,\n",
      "           1.2436,  1.4090, -1.7305, -1.0663, -0.1203]]])\n",
      "tensor([[[ 0.0731,  0.0318,  0.2228,  0.1312]],\n",
      "\n",
      "        [[ 0.1244,  0.0051, -0.0490,  0.3171]],\n",
      "\n",
      "        [[ 0.2299,  0.1544,  0.5173,  0.0486]],\n",
      "\n",
      "        [[ 0.4352,  0.1513,  0.2396, -0.0255]],\n",
      "\n",
      "        [[ 0.0268,  0.2064,  0.5592,  0.2090]]], grad_fn=<CatBackward>)\n",
      "(tensor([[[0.0268, 0.2064, 0.5592, 0.2090]]], grad_fn=<ViewBackward>), tensor([[[0.2623, 0.3037, 0.8309, 0.6050]]], grad_fn=<ViewBackward>))\n"
     ]
    }
   ],
   "source": [
    "# LETTERS LSTM EXAMPLE\n",
    "\n",
    "\n",
    "# letters have an embedding space of dim 26 and the LSTM returns an embedding of dim say 4\n",
    "lstm = nn.LSTM(26, 4) \n",
    "inputs = [torch.randn(1, 26) for _ in range(5)]  # make a word of length 5 letters\n",
    "print(inputs)\n",
    "\n",
    "# initialize the hidden state.\n",
    "hidden = (torch.randn(1, 1, 4),\n",
    "          torch.randn(1, 1, 4))\n",
    "\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "\n",
    "## or all at once\n",
    "\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "print(inputs)\n",
    "# clean out hidden state like above\n",
    "hidden = (torch.randn(1, 1, 4),\n",
    "          torch.randn(1, 1, 4))  \n",
    "\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)\n",
    "\n",
    "# the hidden layer will be our character level embedding\n",
    "# it will be concatenated to our word embedding, before being fed into the sentence level LSTM\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 617,
     "status": "ok",
     "timestamp": 1588321024222,
     "user": {
      "displayName": "Skander Ben Mansour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggl7domjq2nPtW4SYKR5BEYuj4qkl2Ljge9t-vlBiU=s64",
      "userId": "11506237701449758837"
     },
     "user_tz": -60
    },
    "id": "bDnaPl4RK2Fu",
    "outputId": "07175cf9-da3e-405c-a09c-97b7f23b7352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25}\n",
      "{'the': 0, 'dog': 1, 'ate': 2, 'apple': 3, 'everybody': 4, 'read': 5, 'that': 6, 'book': 7}\n",
      "{'DET': 0, 'V': 1, 'NN': 2}\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    (\"The dog ate the apple\".lower().split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".lower().split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "\n",
    "letter_id = dict(zip(string.ascii_letters[:26],range(26)))\n",
    "\n",
    "word_id = defaultdict(lambda:len(word_id))\n",
    "all_tags = set()\n",
    "\n",
    "for sent, tags in training_data:\n",
    "    all_tags |= set(tags)\n",
    "    for word in sent:\n",
    "        word_id[word]\n",
    "tag_id = dict(zip(all_tags,range(len(all_tags))))\n",
    "word_id = dict(word_id)\n",
    "\n",
    "print(letter_id)\n",
    "print(word_id)\n",
    "print(tag_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MsZymdAKDRAB"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def get_letter_vector(letter):\n",
    "    v = np.zeros(26)\n",
    "    v[letter_id[letter]] = 1\n",
    "    return torch.tensor(v,dtype=torch.long)\n",
    "\n",
    "def get_word_vector(word):\n",
    "  return torch.cat([get_letter_vector(letter) for letter in word]).view(-1,26)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 861,
     "status": "ok",
     "timestamp": 1588327479251,
     "user": {
      "displayName": "Skander Ben Mansour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggl7domjq2nPtW4SYKR5BEYuj4qkl2Ljge9t-vlBiU=s64",
      "userId": "11506237701449758837"
     },
     "user_tz": -60
    },
    "id": "0l_VaRV5N9Ld",
    "outputId": "76d836f3-c6a4-4503-aaf4-2098ae2c974e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_sequence('the ate'.split(),word_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 633,
     "status": "ok",
     "timestamp": 1588321035480,
     "user": {
      "displayName": "Skander Ben Mansour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggl7domjq2nPtW4SYKR5BEYuj4qkl2Ljge9t-vlBiU=s64",
      "userId": "11506237701449758837"
     },
     "user_tz": -60
    },
    "id": "dcQIiqqgM_dE",
    "outputId": "48a8c4f3-b46e-4d0a-ac16-04ad1c4c873e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_letter_vector('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 581,
     "status": "ok",
     "timestamp": 1588327482433,
     "user": {
      "displayName": "Skander Ben Mansour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggl7domjq2nPtW4SYKR5BEYuj4qkl2Ljge9t-vlBiU=s64",
      "userId": "11506237701449758837"
     },
     "user_tz": -60
    },
    "id": "aNl6nK_kNdkW",
    "outputId": "bc11b2b2-bdb7-4ed5-cf78-f2572b205573"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_vector('ab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1A9feXpDKRSr"
   },
   "outputs": [],
   "source": [
    "N_LETTERS = 26\n",
    "\n",
    "WORD_EMBEDDING_SIZE = 6\n",
    "HIDDEN_DIM_LETTER_LSTM = 5\n",
    "TOTAL_EMBEDDING_SIZE = WORD_EMBEDDING_SIZE + HIDDEN_DIM_LETTER_LSTM\n",
    "\n",
    "HIDDEN_DIM_FULL_LSTM = 6\n",
    "\n",
    "VOCAB_SIZE = len(word_id)\n",
    "\n",
    "TAGSET_SIZE = len(tag_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QkMXsMRupHzc"
   },
   "outputs": [],
   "source": [
    "def init_hidden(dim):\n",
    "        return (torch.zeros(1, 1, dim),\n",
    "                torch.zeros(1, 1, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 657,
     "status": "ok",
     "timestamp": 1588328478120,
     "user": {
      "displayName": "Skander Ben Mansour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggl7domjq2nPtW4SYKR5BEYuj4qkl2Ljge9t-vlBiU=s64",
      "userId": "11506237701449758837"
     },
     "user_tz": -60
    },
    "id": "Xzubr5h7nl_1",
    "outputId": "8f42a889-b99b-4248-ea38-77ba4946dc27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 0, 3])\n",
      "tensor([[-0.5517,  0.8091, -0.4110,  0.5568,  0.5924,  0.4504],\n",
      "        [ 1.0375,  0.0895, -1.9281,  0.4494,  0.8918,  0.7742],\n",
      "        [-0.9697, -0.6761,  0.7772,  0.6192,  1.6007, -0.9435],\n",
      "        [-0.5517,  0.8091, -0.4110,  0.5568,  0.5924,  0.4504],\n",
      "        [-1.1967, -0.4981, -0.4123,  0.6369,  0.9843, -1.1036]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "tensor([[[ 0.1472,  0.3769, -0.4417,  0.3877,  0.1941,  0.0209]]],\n",
      "       grad_fn=<CatBackward>)\n",
      "(tensor([[[ 0.1472,  0.3769, -0.4417,  0.3877,  0.1941,  0.0209]]],\n",
      "       grad_fn=<ViewBackward>), tensor([[[ 0.2729,  0.6183, -0.6431,  0.7131,  0.2420,  0.0385]]],\n",
      "       grad_fn=<ViewBackward>))\n"
     ]
    }
   ],
   "source": [
    "## Try the simple LSTM by hand (no letter embedding)\n",
    "\n",
    "\n",
    "nn_word_embeddings = nn.Embedding(VOCAB_SIZE, WORD_EMBEDDING_SIZE)\n",
    "lstm_full = nn.LSTM(WORD_EMBEDDING_SIZE, HIDDEN_DIM_FULL_LSTM)  \n",
    "hidden_full = init_hidden(HIDDEN_DIM_FULL_LSTM)\n",
    "\n",
    "sentence = 'the dog ate the apple'.split()\n",
    "n_words = len(sentence)\n",
    "word_ids = prepare_sequence(sentence,word_id)\n",
    "print(word_ids)\n",
    "word_embeddings = nn_word_embeddings(word_ids)\n",
    "print(word_embeddings)\n",
    "\n",
    "\n",
    "for word_embedding in word_embeddings:\n",
    "    print(word_embedding.dtype)\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    out, hidden_full = lstm_full(word_embedding.view(1, 1, -1), hidden_full)\n",
    "\n",
    "print(out)\n",
    "# out is our final embedding\n",
    "print(hidden_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 557,
     "status": "ok",
     "timestamp": 1588327708889,
     "user": {
      "displayName": "Skander Ben Mansour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggl7domjq2nPtW4SYKR5BEYuj4qkl2Ljge9t-vlBiU=s64",
      "userId": "11506237701449758837"
     },
     "user_tz": -60
    },
    "id": "XlW6cC9wqqzg",
    "outputId": "3cf7ba6e-c89b-42a3-8faa-8fdfcd2ff088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[[-0.3223, -0.3033,  0.0765, -0.1239,  0.2630]]], grad_fn=<CatBackward>)\n",
      "(tensor([[[-0.3223, -0.3033,  0.0765, -0.1239,  0.2630]]], grad_fn=<ViewBackward>), tensor([[[-0.6339, -0.5351,  0.1922, -0.2950,  0.4478]]], grad_fn=<ViewBackward>))\n"
     ]
    }
   ],
   "source": [
    "## Try the letter LSTM by hand\n",
    "\n",
    "\n",
    "\n",
    "lstm_letters = nn.LSTM(N_LETTERS, HIDDEN_DIM_LETTER_LSTM)  \n",
    "hidden_letters = init_hidden(HIDDEN_DIM_LETTER_LSTM)\n",
    "\n",
    "word = 'orange'\n",
    "n_letters = len(word)\n",
    "letter_ids = get_word_vector(word).type(torch.FloatTensor)\n",
    "print(letter_ids)\n",
    "\n",
    "\n",
    "for letter in letter_ids:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    letter_out, hidden_letters = lstm_letters(letter.view(1, 1, -1), hidden_letters)\n",
    "\n",
    "print(letter_out)\n",
    "# letter_out is our letter level embedding for the word\n",
    "print(hidden_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 543,
     "status": "ok",
     "timestamp": 1588328496858,
     "user": {
      "displayName": "Skander Ben Mansour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggl7domjq2nPtW4SYKR5BEYuj4qkl2Ljge9t-vlBiU=s64",
      "userId": "11506237701449758837"
     },
     "user_tz": -60
    },
    "id": "79WIt48ytHow",
    "outputId": "718709ed-3d1c-463b-a349-c100213971dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 0, 3])\n",
      "tensor([[-0.1382,  0.6345, -0.2083, -0.0995,  1.3494, -0.5304],\n",
      "        [-0.0582, -1.3537,  0.6772,  0.6895,  0.2098,  1.0609],\n",
      "        [ 0.1944,  0.2364,  0.4106,  1.9754,  1.5703, -1.1593],\n",
      "        [-0.1382,  0.6345, -0.2083, -0.0995,  1.3494, -0.5304],\n",
      "        [-1.4360, -0.0371, -2.2927, -1.7063,  0.0197, -1.6556]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "[tensor([[-0.1716,  0.1784,  0.0308,  0.0682, -0.0352,  0.1832]],\n",
      "       grad_fn=<SelectBackward>), tensor([[-0.0274,  0.2264,  0.1154,  0.0255, -0.0347, -0.0906]],\n",
      "       grad_fn=<SelectBackward>), tensor([[-0.1173,  0.2796,  0.1380, -0.0939, -0.2960,  0.1391]],\n",
      "       grad_fn=<SelectBackward>), tensor([[-0.2909,  0.2802,  0.1335, -0.0009, -0.2475,  0.2279]],\n",
      "       grad_fn=<SelectBackward>), tensor([[-0.2872,  0.1694,  0.2392,  0.4215, -0.4810,  0.1172]],\n",
      "       grad_fn=<SelectBackward>)]\n"
     ]
    }
   ],
   "source": [
    "## Putting it all together\n",
    "\n",
    "\n",
    "\n",
    "lstm_letters = nn.LSTM(N_LETTERS, HIDDEN_DIM_LETTER_LSTM)  \n",
    "hidden_letters = init_hidden(HIDDEN_DIM_LETTER_LSTM)\n",
    "\n",
    "nn_word_embeddings = nn.Embedding(VOCAB_SIZE, WORD_EMBEDDING_SIZE)\n",
    "# the full LSTM now takes a larger input : concatenating the original word embedding + letter level embedding (output of first LSTM)\n",
    "lstm_full = nn.LSTM(WORD_EMBEDDING_SIZE+HIDDEN_DIM_LETTER_LSTM, HIDDEN_DIM_FULL_LSTM)  \n",
    "hidden_full = init_hidden(HIDDEN_DIM_FULL_LSTM)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sentence = 'the dog ate the apple'.split()\n",
    "n_words = len(sentence)\n",
    "word_ids = prepare_sequence(sentence,word_id)\n",
    "print(word_ids)\n",
    "word_embeddings = nn_word_embeddings(word_ids)\n",
    "print(word_embeddings)\n",
    "    \n",
    "all_outs = []\n",
    "\n",
    "for pos in range(n_words):\n",
    "    word_embedding = word_embeddings[pos]\n",
    "    print(word_embedding.dtype)\n",
    "    \n",
    "    word = sentence[pos]\n",
    "    n_letters = len(word)\n",
    "    letter_ids = get_word_vector(word).type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "    \n",
    "    for letter in letter_ids:\n",
    "        letter_out, hidden_letters = lstm_letters(letter.view(1, 1, -1), hidden_letters)\n",
    "\n",
    "    # we prepare to concatenate the word embedding and letter level embedding\n",
    "    letter_out = letter_out.view(-1,1)\n",
    "    word_embedding = word_embedding.view(-1,1)\n",
    "    full_lstm_input = torch.cat([word_embedding,letter_out])\n",
    "\n",
    "    # and can then feed this into the full LSTM\n",
    "    out, hidden_full = lstm_full(full_lstm_input.view(1, 1, -1), hidden_full)\n",
    "    all_outs.append(out[-1])\n",
    "\n",
    "\n",
    "# out is our final embedding\n",
    "print(all_outs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yMQXJhfFDRAJ"
   },
   "source": [
    "Create the model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZxLqCM84DRAL"
   },
   "outputs": [],
   "source": [
    "class CharLevelLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CharLevelLSTM, self).__init__()\n",
    "\n",
    "        self.lstm_letters = nn.LSTM(N_LETTERS, HIDDEN_DIM_LETTER_LSTM)  \n",
    "        self.hidden_letters = init_hidden(HIDDEN_DIM_LETTER_LSTM)\n",
    "\n",
    "        self.nn_word_embeddings = nn.Embedding(VOCAB_SIZE, WORD_EMBEDDING_SIZE)\n",
    "        # the full LSTM now takes a larger input : concatenating the original word embedding + letter level embedding (output of first LSTM)\n",
    "        self.lstm_full = nn.LSTM(WORD_EMBEDDING_SIZE+HIDDEN_DIM_LETTER_LSTM, HIDDEN_DIM_FULL_LSTM)  \n",
    "        self.hidden_full = init_hidden(HIDDEN_DIM_FULL_LSTM)\n",
    "\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(HIDDEN_DIM_FULL_LSTM, TAGSET_SIZE)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # note the input is now actual words, not word_ids\n",
    "\n",
    "        n_words = len(sentence)\n",
    "\n",
    "        word_ids = prepare_sequence(sentence,word_id)\n",
    "        # print(word_ids)\n",
    "        word_embeddings = self.nn_word_embeddings(word_ids)\n",
    "        # print(word_embeddings)\n",
    "\n",
    "        all_outs = []\n",
    "        \n",
    "        for pos in range(n_words):\n",
    "            word_embedding = word_embeddings[pos]\n",
    "            # print(word_embedding.dtype)\n",
    "            \n",
    "            word = sentence[pos]\n",
    "            n_letters = len(word)\n",
    "            letter_ids = get_word_vector(word).type(torch.FloatTensor)\n",
    "            \n",
    "            # if we want each character level word embedding to be independent from the previous one\n",
    "            self.hidden_letters = init_hidden(HIDDEN_DIM_LETTER_LSTM)\n",
    "            \n",
    "            for letter in letter_ids:\n",
    "                letter_out, self.hidden_letters = self.lstm_letters(letter.view(1, 1, -1), self.hidden_letters)\n",
    "\n",
    "            # we prepare to concatenate the word embedding and letter level embedding\n",
    "            letter_out = letter_out.view(-1,1)\n",
    "            word_embedding = word_embedding.view(-1,1)\n",
    "            full_lstm_input = torch.cat([word_embedding,letter_out])\n",
    "\n",
    "            # and can then feed this into the full LSTM\n",
    "            out, self.hidden_full = self.lstm_full(full_lstm_input.view(1, 1, -1), self.hidden_full)\n",
    "            \n",
    "            all_outs.append(out[-1])\n",
    "\n",
    "            \n",
    "        all_outs = torch.cat(all_outs)\n",
    "        # print(out)\n",
    "        # out is our final embedding\n",
    "        # print(hidden_full)\n",
    "\n",
    "        tag_space = self.hidden2tag(all_outs.view(n_words, -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 542,
     "status": "ok",
     "timestamp": 1588328751367,
     "user": {
      "displayName": "Skander Ben Mansour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggl7domjq2nPtW4SYKR5BEYuj4qkl2Ljge9t-vlBiU=s64",
      "userId": "11506237701449758837"
     },
     "user_tz": -60
    },
    "id": "B_xLRqENxUmX",
    "outputId": "ce228119-6507-4c2e-c94c-42648824423a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'dog', 'ate', 'the', 'apple']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xQL824RODRAT"
   },
   "source": [
    "Train the model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 510,
     "status": "error",
     "timestamp": 1588328788770,
     "user": {
      "displayName": "Skander Ben Mansour",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggl7domjq2nPtW4SYKR5BEYuj4qkl2Ljge9t-vlBiU=s64",
      "userId": "11506237701449758837"
     },
     "user_tz": -60
    },
    "id": "UJD0omS4DRAV",
    "outputId": "7505a358-aeef-46d6-f72f-a41586029b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8660, -1.3830, -1.1130],\n",
      "        [-0.8799, -1.2933, -1.1686],\n",
      "        [-0.8404, -1.3016, -1.2162],\n",
      "        [-0.8964, -1.2955, -1.1450],\n",
      "        [-0.9046, -1.3236, -1.1113]])\n"
     ]
    }
   ],
   "source": [
    "model = CharLevelLSTM()\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    tag_scores = model(training_data[0][0])\n",
    "    print(tag_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n61pmVVaTmmk"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c8bbd001ed45f4b825cb991f8ad8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = CharLevelLSTM()\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(300)):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden_full = init_hidden(HIDDEN_DIM_FULL_LSTM)\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        targets = prepare_sequence(tags, tag_id)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0266, -4.5385, -4.1618],\n",
      "        [-4.5295, -5.0415, -0.0174],\n",
      "        [-2.9534, -0.0713, -4.0976],\n",
      "        [-0.0220, -4.3912, -4.6667],\n",
      "        [-4.1935, -4.9556, -0.0224]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_id)\n",
    "    tag_scores = model(training_data[0][0])\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DET'), ('dog', 'NN'), ('ate', 'V'), ('the', 'DET'), ('apple', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inverse_tag_id = {v:k for k,v in tag_id.items()}\n",
    "\n",
    "\n",
    "print(list(zip(training_data[0][0],(map(inverse_tag_id.get,tag_scores.numpy().argmax(axis=1))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NN', 'V', 'DET', 'NN']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM_char_level.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/sequence_models_tutorial.ipynb",
     "timestamp": 1588270333019
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
