{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "source is https://www.kaggle.com/c/word2vec-nlp-tutorial/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/tyler/Documents/programming/pytorch_nlp/data/word2vec-nlp-tutorial/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path+'labeledTrainData.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[:15000]\n",
    "val = df[15000:20000]\n",
    "test = df[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 5000, 5000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train),len(val),len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_review(review):\n",
    "    chars = ['/','\\\\','>','<','-','br']\n",
    "    chars.extend('1 2 3 4 5 6 7 8 9 0'.split())\n",
    "    for char in chars:\n",
    "        review = review.replace(char,'')\n",
    "    \n",
    "    tokens = word_tokenize(review)\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make vocab and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tyler/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb5c53d77434c6bb26cc17b6f5c9bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labels = list(train.sentiment)\n",
    "reviews = list(train.review.values)\n",
    "\n",
    "all_words = [process_review(review) for review in tqdm(reviews)]\n",
    "\n",
    "train_data = list(zip(all_words,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in all_words for item in sublist]\n",
    "vocab = set(flat_list)\n",
    "\n",
    "len(vocab)\n",
    "\n",
    "word_to_idx = {word:idx for idx,word in enumerate(list(vocab))}\n",
    "\n",
    "counts = Counter(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(flat_list).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 10\n",
    "keep = counts[start:20000+start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 52046), ('this', 45732), ('that', 44178), (\"'s\", 37794), ('was', 30368)]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [word for word,count in keep]\n",
    "vocab.append('UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {word:idx for idx,word in enumerate(list(vocab))}\n",
    "idx_to_word = {idx:word for word,idx in word_to_idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed10e17bec79478880668f5294ba4166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labels = list(test.sentiment)\n",
    "reviews = list(test.review.values)\n",
    "\n",
    "all_words = [process_review(review) for review in tqdm(reviews)]\n",
    "\n",
    "test_data = list(zip(all_words,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa70b8c11edc4e0cbffd129bd821aa65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labels = list(val.sentiment)\n",
    "reviews = list(val.review.values)\n",
    "\n",
    "all_words = [process_review(review) for review in tqdm(reviews)]\n",
    "\n",
    "val_data = list(zip(all_words,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW --> Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5813bae96284501b0d4340b54f00eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for sentence, label in tqdm(train_data):\n",
    "    for i in range(2, len(sentence) - 2):\n",
    "        context = [sentence[i - 2], sentence[i - 1],sentence[i + 1], sentence[i + 2]]\n",
    "        target = sentence[i]\n",
    "        data.append((context, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_data = []\n",
    "\n",
    "for context,target_word in data:\n",
    "    inputs = make_context_vector(context, word_to_idx)\n",
    "    target = to_idx(target_word)\n",
    "    tensor_data.append((inputs,target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(tensor_data, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size,batch_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        embeds_sum = embeds.sum(dim=1)\n",
    "        out = F.relu(self.linear1(embeds_sum))\n",
    "        out = self.linear2(out)\n",
    "        out = out.view(batch_size,-1)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_idx(w):\n",
    "    idx = word_to_idx.get(w)\n",
    "    if idx is None:\n",
    "        idx = 20000\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context_vector(context, word_to_idx):\n",
    "    \n",
    "    idxs = [to_idx(w) for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab) + 1\n",
    "embedding_dim = 10\n",
    "context_size = 4\n",
    "batch_size = 1000\n",
    "cbow = CBOW(vocab_size, embedding_dim, context_size,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(cbow.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: train loss of 8.339\n",
      "Epoch 2/10: train loss of 8.000\n",
      "Epoch 3/10: train loss of 8.044\n",
      "Epoch 4/10: train loss of 7.909\n",
      "Epoch 5/10: train loss of 7.744\n",
      "Epoch 6/10: train loss of 7.975\n",
      "Epoch 7/10: train loss of 7.644\n",
      "Epoch 8/10: train loss of 7.706\n",
      "Epoch 9/10: train loss of 7.805\n",
      "Epoch 10/10: train loss of 7.698\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    idx = 0\n",
    "    for context,target_word in data_loader:\n",
    "        if idx  < 200:\n",
    "            #inputs = make_context_vector(context, word_to_idx)\n",
    "            #target = torch.tensor([word_to_idx[target_word]], dtype=torch.long)\n",
    "            inputs = context\n",
    "            target = target_word\n",
    "            cbow.zero_grad()\n",
    "\n",
    "            log_probs = cbow(inputs)\n",
    "            loss = loss_function(log_probs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            idx += 1\n",
    "\n",
    "    train_loss.append(loss.item())\n",
    "    mean_train_loss = np.mean(train_loss)\n",
    "    losses.append(round(mean_train_loss,2))\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}: train loss of {mean_train_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who still [UNK] UNK funny ==> have\n",
      "been told [UNK] UNK film ==> that\n",
      "adventure movie [UNK] but UNK ==> UNK\n",
      "UNK UNK [UNK] UNK UNK ==> group\n",
      "' behindthescenes [UNK] UNK UNK ==> short\n",
      "( inappropriately [UNK] `` cowboy ==> named\n",
      "aspects UNK [UNK] UNK niven ==> UNK\n",
      "UNK h [UNK] UNK lot ==> showcases\n",
      "effect UNK [UNK] nicholson does ==> director\n",
      "ancient grandfather [UNK] his dogs ==> UNK\n",
      "UNK UNK [UNK] cast mixing ==> great\n",
      "UNK as [UNK] cg movies ==> UNK\n",
      "i used [UNK] say that ==> UNK\n",
      "apart from [UNK] UNK UNK ==> that\n",
      "UNK get [UNK] better UNK ==> UNK\n",
      "make UNK [UNK] statement UNK ==> comical\n",
      "another dud [UNK] ha ha ==> (\n",
      "could develop [UNK] connection with ==> UNK\n",
      "UNK surface [UNK] you could ==> UNK\n",
      "each step [UNK] UNK way ==> UNK\n"
     ]
    }
   ],
   "source": [
    "for context,target_word in data_loader:\n",
    "    inputs = context\n",
    "    target = target_word\n",
    "    cbow.eval()\n",
    "    log_probs = cbow(inputs)\n",
    "    argmax = log_probs.argmax(dim=1)\n",
    "    \n",
    "    for i in range(20):\n",
    "        c = context[i]\n",
    "        c1 = idx_to_word[int(c[0])]\n",
    "        c2 = idx_to_word[int(c[1])]\n",
    "        c3 = idx_to_word[int(c[2])]\n",
    "        c4 = idx_to_word[int(c[3])]\n",
    "        prediction = idx_to_word[int(argmax[i])]\n",
    "        t = idx_to_word[int(target[i])]\n",
    "        print(f'{c1} {c2} [{prediction}] {c3} {c4} ==> {t}')\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'model_checkpoints/embeddings.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cbow.state_dict(),save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = cbow.embeddings.weight.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings --> Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target(label, label_to_idx):\n",
    "    return torch.LongTensor([label_to_idx[label]])\n",
    "\n",
    "def make_bow_vector(sentence, word_to_idx):\n",
    "    vec = torch.zeros(len(word_to_idx),dtype=torch.long)\n",
    "    for word in sentence:\n",
    "        if word in word_to_idx:\n",
    "            vec[word_to_idx[word]] += 1\n",
    "    return vec.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class embed_classifier(nn.Module):\n",
    "    def __init__(self, num_labels, vocab_size, embedding_dim,hidden):\n",
    "        super(embed_classifier, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, hidden)\n",
    "        self.linear_2 = nn.Linear(hidden, num_labels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        embeds_sum = embeds.sum(dim=1)\n",
    "        x = self.linear(embeds_sum)\n",
    "        #rint(x.shape)\n",
    "        x = self.linear_2(x)\n",
    "        #print(x.shape)\n",
    "        return F.log_softmax((x), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_to_idx) + 1\n",
    "NUM_LABELS = 2\n",
    "embedding_dim = 10\n",
    "hidden = 10\n",
    "model = embed_classifier(NUM_LABELS, VOCAB_SIZE,embedding_dim,hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20001"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embeddings.weight.data.copy_(torch.tensor(weights))\n",
    "model.embeddings.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "lr = .001\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2718.1111,     0.0000]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    sample = all_words[4]\n",
    "    bow_vector = make_bow_vector(sample[:20], word_to_idx)\n",
    "    log_probs = model(bow_vector)\n",
    "    print(log_probs)\n",
    "    loss = -loss_function(log_probs, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: train loss of 31.966, val loss of 2.189\n",
      "Val loss decreased inf --> 2.189 saving model...\n",
      "Epoch 2/10: train loss of 1.607, val loss of 0.736\n",
      "Val loss decreased 2.189 --> 0.736 saving model...\n",
      "Epoch 3/10: train loss of 1.135, val loss of 0.694\n",
      "Val loss decreased 0.736 --> 0.694 saving model...\n",
      "Epoch 4/10: train loss of 1.146, val loss of 0.699\n",
      "Epoch 5/10: train loss of 1.094, val loss of 0.738\n",
      "Epoch 6/10: train loss of 1.148, val loss of 0.711\n",
      "Epoch 7/10: train loss of 1.111, val loss of 0.728\n",
      "Epoch 8/10: train loss of 1.102, val loss of 0.701\n",
      "Epoch 9/10: train loss of 1.114, val loss of 0.707\n",
      "Epoch 10/10: train loss of 1.099, val loss of 0.695\n"
     ]
    }
   ],
   "source": [
    "save_path = 'model_checkpoints/embed_classifier.pt'\n",
    "val_loss_min = np.Inf\n",
    "\n",
    "num_epochs = 10\n",
    "loss_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for sentence, label in train_data:\n",
    "        model.zero_grad()\n",
    "\n",
    "        vec = make_bow_vector(sentence, word_to_idx)\n",
    "        vec = torch.LongTensor(vec)\n",
    "        target = torch.LongTensor([label])\n",
    "\n",
    "        log_probs = model(vec)\n",
    "\n",
    "        loss = loss_function(log_probs, target)\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    mean_train_loss = np.mean(train_loss)\n",
    "    val_loss = []\n",
    "    for sentence, label in val_data:\n",
    "        model.eval()\n",
    "\n",
    "        vec = make_bow_vector(sentence, word_to_idx)\n",
    "        target = torch.LongTensor([label])\n",
    "\n",
    "        log_probs = model(vec)\n",
    "        pred = log_probs.argmax().detach().numpy()\n",
    "        loss = loss_function(log_probs, target)\n",
    "\n",
    "        val_loss.append(loss.item())\n",
    "        \n",
    "    mean_val_loss = np.mean(val_loss)\n",
    "    \n",
    "    loss_history.append((mean_train_loss,mean_val_loss))\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}: train loss of {mean_train_loss:.3f}, val loss of {mean_val_loss:.3f}')\n",
    "    \n",
    "    if mean_val_loss <= val_loss_min:\n",
    "        print(f'Val loss decreased {val_loss_min:.3f} --> {mean_val_loss:.3f} saving model...')\n",
    "        torch.save(model.state_dict(),save_path)\n",
    "        val_loss_min = mean_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----TRAIN SET----\n",
      "train loss of 0.695\n",
      "train accuracy of 50.09\n",
      "----VAL SET----\n",
      "val loss of 0.695\n",
      "val accuracy of 50.3\n",
      "----TEST SET----\n",
      "test loss of 0.696\n",
      "test accuracy of 49.44\n"
     ]
    }
   ],
   "source": [
    "names = 'train val test'.split()\n",
    "data_list = [train_data,val_data,test_data]\n",
    "\n",
    "for name,data in zip(names,data_list):\n",
    "    eval_loss = []\n",
    "    num_correct = 0\n",
    "    to_eval = test_data\n",
    "    for sentence, label in data:\n",
    "        model.eval()\n",
    "\n",
    "        vec = make_bow_vector(sentence, word_to_idx)\n",
    "        target = torch.LongTensor([label])\n",
    "\n",
    "        log_probs = model(vec)\n",
    "        pred = log_probs.argmax().detach().numpy()\n",
    "        correct = int(pred == label)\n",
    "        num_correct += correct\n",
    "        loss = loss_function(log_probs, target)\n",
    "\n",
    "        eval_loss.append(loss.item())\n",
    "    \n",
    "    mean_loss = np.mean(eval_loss)\n",
    "    print(f'----{name} set----'.upper())\n",
    "    print(f'{name} loss of {round(mean_loss,3)}')\n",
    "    print(f'{name} accuracy of {round(num_correct*100/len(data),2)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
