{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Copy of Copy of sequence_models_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skanderbenmansour/nlp_study_group/blob/master/skander/skander_bug_fixes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8X5BSymDQ_w",
        "colab_type": "code",
        "outputId": "dfe3cb08-32af-41bd-9c0a-eea3ac916592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import string\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f22f81eb070>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64VoojxzDQ_5",
        "colab_type": "code",
        "outputId": "4a3ec1c0-3936-43a8-aeaf-ae0fdc615782",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "# SENTENCE LSTM EXAMPLE\n",
        "\n",
        "\n",
        "# We imagine we have a  word embedding space of space of dim 10 and the LSTM returns an embedding of dim 5\n",
        "lstm = nn.LSTM(10, 5) \n",
        "inputs = [torch.randn(1, 10) for _ in range(5)]  # make a sentence of length 5\n",
        "print(inputs)\n",
        "\n",
        "# initialize the hidden state.\n",
        "hidden = (torch.randn(1, 1, 5),\n",
        "          torch.randn(1, 1, 5))\n",
        "\n",
        "for i in inputs:\n",
        "    # Step through the sequence one element at a time.\n",
        "    # after each step, hidden contains the hidden state.\n",
        "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
        "\n",
        "## or all at once\n",
        "\n",
        "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
        "print(inputs)\n",
        "# clean out hidden state like above\n",
        "hidden = (torch.randn(1, 1, 5),\n",
        "          torch.randn(1, 1, 5))  \n",
        "\n",
        "out, hidden = lstm(inputs, hidden)\n",
        "print(out)\n",
        "print(hidden)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([[ 1.4697, -0.3951, -0.5101,  1.1163, -0.5926,  0.9089, -1.0792, -0.6751,\n",
            "          1.4083, -1.8456]]), tensor([[-0.5655, -0.9094, -0.4891, -0.6175,  0.3836,  0.3852,  0.7890,  1.1363,\n",
            "         -0.1147, -0.0180]]), tensor([[ 0.0491,  0.4272, -0.8996,  0.5313,  0.4034,  1.4521, -2.4182, -1.1906,\n",
            "          0.6964,  1.1296]]), tensor([[ 0.2214, -0.0558,  1.2057,  1.9486, -0.0766, -0.8562, -0.7870, -0.8161,\n",
            "          0.5470, -1.1707]]), tensor([[-0.4699, -1.6271, -0.1127,  1.5980, -0.8445, -1.0489,  0.9387,  0.5378,\n",
            "          1.5372, -0.6943]])]\n",
            "tensor([[[ 1.4697, -0.3951, -0.5101,  1.1163, -0.5926,  0.9089, -1.0792,\n",
            "          -0.6751,  1.4083, -1.8456]],\n",
            "\n",
            "        [[-0.5655, -0.9094, -0.4891, -0.6175,  0.3836,  0.3852,  0.7890,\n",
            "           1.1363, -0.1147, -0.0180]],\n",
            "\n",
            "        [[ 0.0491,  0.4272, -0.8996,  0.5313,  0.4034,  1.4521, -2.4182,\n",
            "          -1.1906,  0.6964,  1.1296]],\n",
            "\n",
            "        [[ 0.2214, -0.0558,  1.2057,  1.9486, -0.0766, -0.8562, -0.7870,\n",
            "          -0.8161,  0.5470, -1.1707]],\n",
            "\n",
            "        [[-0.4699, -1.6271, -0.1127,  1.5980, -0.8445, -1.0489,  0.9387,\n",
            "           0.5378,  1.5372, -0.6943]]])\n",
            "tensor([[[ 0.5595, -0.0880, -0.1437, -0.4697,  0.2218]],\n",
            "\n",
            "        [[ 0.0775, -0.0024, -0.3386, -0.2280,  0.3244]],\n",
            "\n",
            "        [[-0.0792, -0.1593, -0.0167, -0.3246,  0.0927]],\n",
            "\n",
            "        [[ 0.1874, -0.2819, -0.0086, -0.0823,  0.0315]],\n",
            "\n",
            "        [[ 0.2854, -0.2004, -0.0710, -0.1411,  0.3178]]],\n",
            "       grad_fn=<StackBackward>)\n",
            "(tensor([[[ 0.2854, -0.2004, -0.0710, -0.1411,  0.3178]]],\n",
            "       grad_fn=<StackBackward>), tensor([[[ 0.7698, -0.6869, -0.3282, -0.3147,  0.3889]]],\n",
            "       grad_fn=<StackBackward>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yrgMmhqSnKv",
        "colab_type": "code",
        "outputId": "3d52660d-f954-4097-cec6-4605c6a162ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# LETTERS LSTM EXAMPLE\n",
        "\n",
        "\n",
        "# letters have an embedding space of dim 26 and the LSTM returns an embedding of dim say 4\n",
        "lstm = nn.LSTM(26, 4) \n",
        "inputs = [torch.randn(1, 26) for _ in range(5)]  # make a word of length 5 letters\n",
        "print(inputs)\n",
        "\n",
        "# initialize the hidden state.\n",
        "hidden = (torch.randn(1, 1, 4),\n",
        "          torch.randn(1, 1, 4))\n",
        "\n",
        "for i in inputs:\n",
        "    # Step through the sequence one element at a time.\n",
        "    # after each step, hidden contains the hidden state.\n",
        "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
        "\n",
        "## or all at once\n",
        "\n",
        "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
        "print(inputs)\n",
        "# clean out hidden state like above\n",
        "hidden = (torch.randn(1, 1, 4),\n",
        "          torch.randn(1, 1, 4))  \n",
        "\n",
        "out, hidden = lstm(inputs, hidden)\n",
        "print(out)\n",
        "\n",
        "# the hidden layer will be our character level embedding\n",
        "# it will be concatenated to our word embedding, before being fed into the sentence level LSTM\n",
        "print(hidden)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([[-1.5116e+00,  1.9068e-01,  2.0442e-01,  1.6386e-01, -1.2945e+00,\n",
            "         -1.2855e-01, -5.7129e-02, -7.1059e-02,  1.1658e+00,  1.7011e-01,\n",
            "         -1.0919e+00,  8.2633e-02,  1.3116e-02, -1.1464e+00, -2.6034e-01,\n",
            "         -3.1155e-01,  8.9364e-01, -1.0561e+00, -5.6764e-01,  4.3550e-01,\n",
            "          8.4634e-04, -7.8948e-02,  8.7507e-01,  5.5482e-01, -9.7478e-01,\n",
            "         -5.9503e-01]]), tensor([[ 0.8703,  0.8653,  0.2820,  0.9745,  0.1880, -0.9599, -0.4766, -2.0444,\n",
            "          1.3598, -0.3804,  1.5289, -1.6573,  0.0821,  1.1764,  0.0877, -0.9174,\n",
            "          0.9862,  0.4376,  0.0470, -0.0343, -0.0269, -0.3772,  0.2464,  0.5727,\n",
            "          0.5574,  0.3598]]), tensor([[ 9.3144e-01, -2.4962e+00,  7.9221e-01,  1.1106e+00,  1.3894e-01,\n",
            "          1.1380e+00,  5.0487e-01, -1.1455e+00, -1.8463e-01, -1.4921e+00,\n",
            "          4.9979e-01, -8.1991e-01,  4.2095e-01, -1.7431e+00, -1.6584e+00,\n",
            "          1.7252e+00,  1.3536e+00,  1.6513e+00,  1.1118e-03, -8.3201e-01,\n",
            "          6.3567e-01, -7.7746e-01, -5.5710e-02, -4.4338e-02,  2.0859e+00,\n",
            "          1.0513e-01]]), tensor([[ 0.1401, -1.0772,  1.1557, -1.0234,  0.9199,  1.3019,  0.9390,  0.8462,\n",
            "          0.9443, -0.7599,  0.6024,  0.1352, -0.6365,  0.0678,  1.0910, -0.1847,\n",
            "          0.0786, -0.8521,  1.1760,  0.9729,  0.0173,  0.7971, -0.2596,  0.3957,\n",
            "          0.4421,  0.9448]]), tensor([[ 0.3672,  1.5841, -2.3037,  0.1520,  0.1131, -1.4705, -0.5454,  0.0985,\n",
            "          0.2416,  0.5700, -1.4720,  1.2683,  0.7452, -0.3816, -1.8310, -0.5801,\n",
            "         -0.4347,  0.7958, -0.3111, -1.0572, -1.6718,  1.2436,  1.4090, -1.7305,\n",
            "         -1.0663, -0.1203]])]\n",
            "tensor([[[-1.5116e+00,  1.9068e-01,  2.0442e-01,  1.6386e-01, -1.2945e+00,\n",
            "          -1.2855e-01, -5.7129e-02, -7.1059e-02,  1.1658e+00,  1.7011e-01,\n",
            "          -1.0919e+00,  8.2633e-02,  1.3116e-02, -1.1464e+00, -2.6034e-01,\n",
            "          -3.1155e-01,  8.9364e-01, -1.0561e+00, -5.6764e-01,  4.3550e-01,\n",
            "           8.4634e-04, -7.8948e-02,  8.7507e-01,  5.5482e-01, -9.7478e-01,\n",
            "          -5.9503e-01]],\n",
            "\n",
            "        [[ 8.7027e-01,  8.6527e-01,  2.8196e-01,  9.7453e-01,  1.8803e-01,\n",
            "          -9.5988e-01, -4.7664e-01, -2.0444e+00,  1.3598e+00, -3.8040e-01,\n",
            "           1.5289e+00, -1.6573e+00,  8.2075e-02,  1.1764e+00,  8.7682e-02,\n",
            "          -9.1739e-01,  9.8618e-01,  4.3756e-01,  4.6966e-02, -3.4273e-02,\n",
            "          -2.6868e-02, -3.7725e-01,  2.4640e-01,  5.7270e-01,  5.5741e-01,\n",
            "           3.5978e-01]],\n",
            "\n",
            "        [[ 9.3144e-01, -2.4962e+00,  7.9221e-01,  1.1106e+00,  1.3894e-01,\n",
            "           1.1380e+00,  5.0487e-01, -1.1455e+00, -1.8463e-01, -1.4921e+00,\n",
            "           4.9979e-01, -8.1991e-01,  4.2095e-01, -1.7431e+00, -1.6584e+00,\n",
            "           1.7252e+00,  1.3536e+00,  1.6513e+00,  1.1118e-03, -8.3201e-01,\n",
            "           6.3567e-01, -7.7746e-01, -5.5710e-02, -4.4338e-02,  2.0859e+00,\n",
            "           1.0513e-01]],\n",
            "\n",
            "        [[ 1.4006e-01, -1.0772e+00,  1.1557e+00, -1.0234e+00,  9.1988e-01,\n",
            "           1.3019e+00,  9.3903e-01,  8.4625e-01,  9.4428e-01, -7.5985e-01,\n",
            "           6.0237e-01,  1.3517e-01, -6.3647e-01,  6.7783e-02,  1.0910e+00,\n",
            "          -1.8469e-01,  7.8630e-02, -8.5212e-01,  1.1760e+00,  9.7286e-01,\n",
            "           1.7296e-02,  7.9711e-01, -2.5958e-01,  3.9571e-01,  4.4208e-01,\n",
            "           9.4482e-01]],\n",
            "\n",
            "        [[ 3.6716e-01,  1.5841e+00, -2.3037e+00,  1.5200e-01,  1.1313e-01,\n",
            "          -1.4705e+00, -5.4536e-01,  9.8454e-02,  2.4157e-01,  5.7004e-01,\n",
            "          -1.4720e+00,  1.2683e+00,  7.4523e-01, -3.8162e-01, -1.8310e+00,\n",
            "          -5.8011e-01, -4.3471e-01,  7.9582e-01, -3.1107e-01, -1.0572e+00,\n",
            "          -1.6718e+00,  1.2436e+00,  1.4090e+00, -1.7305e+00, -1.0663e+00,\n",
            "          -1.2031e-01]]])\n",
            "tensor([[[ 0.0731,  0.0318,  0.2228,  0.1312]],\n",
            "\n",
            "        [[ 0.1244,  0.0051, -0.0490,  0.3171]],\n",
            "\n",
            "        [[ 0.2299,  0.1544,  0.5173,  0.0486]],\n",
            "\n",
            "        [[ 0.4352,  0.1513,  0.2396, -0.0255]],\n",
            "\n",
            "        [[ 0.0268,  0.2064,  0.5592,  0.2090]]], grad_fn=<StackBackward>)\n",
            "(tensor([[[0.0268, 0.2064, 0.5592, 0.2090]]], grad_fn=<StackBackward>), tensor([[[0.2623, 0.3037, 0.8309, 0.6050]]], grad_fn=<StackBackward>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDnaPl4RK2Fu",
        "colab_type": "code",
        "outputId": "5f29e71b-c94d-4d0a-ef95-327695811bf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "training_data = [\n",
        "    (\"The dog ate the apple\".lower().split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "    (\"Everybody read that book\".lower().split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
        "]\n",
        "\n",
        "letter_id = dict(zip(string.ascii_letters[:26],range(26)))\n",
        "\n",
        "word_id = defaultdict(lambda:len(word_id))\n",
        "all_tags = set()\n",
        "\n",
        "for sent, tags in training_data:\n",
        "    all_tags |= set(tags)\n",
        "    for word in sent:\n",
        "        word_id[word]\n",
        "tag_id = dict(zip(all_tags,range(len(all_tags))))\n",
        "word_id = dict(word_id)\n",
        "\n",
        "print(letter_id)\n",
        "print(word_id)\n",
        "print(tag_id)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25}\n",
            "{'the': 0, 'dog': 1, 'ate': 2, 'apple': 3, 'everybody': 4, 'read': 5, 'that': 6, 'book': 7}\n",
            "{'NN': 0, 'DET': 1, 'V': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsZymdAKDRAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "def get_letter_vector(letter):\n",
        "    v = np.zeros(26)\n",
        "    v[letter_id[letter]] = 1\n",
        "    return torch.tensor(v,dtype=torch.long)\n",
        "\n",
        "def get_word_vector(word):\n",
        "  return torch.cat([get_letter_vector(letter) for letter in word]).view(-1,26)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l_VaRV5N9Ld",
        "colab_type": "code",
        "outputId": "704bd1d7-5edc-4805-bb05-d42438a3c6c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "prepare_sequence('the ate'.split(),word_id)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcQIiqqgM_dE",
        "colab_type": "code",
        "outputId": "bcc9593c-dc3c-4b06-ad30-1c1734bed14f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "get_letter_vector('b')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNl6nK_kNdkW",
        "colab_type": "code",
        "outputId": "c325889a-4541-4329-b3ee-eb172e44362e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "get_word_vector('ab')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0],\n",
              "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A9feXpDKRSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_LETTERS = 26\n",
        "\n",
        "WORD_EMBEDDING_SIZE = 6\n",
        "HIDDEN_DIM_LETTER_LSTM = 5\n",
        "TOTAL_EMBEDDING_SIZE = WORD_EMBEDDING_SIZE + HIDDEN_DIM_LETTER_LSTM\n",
        "\n",
        "HIDDEN_DIM_FULL_LSTM = 6\n",
        "\n",
        "VOCAB_SIZE = len(word_id)\n",
        "\n",
        "TAGSET_SIZE = len(tag_id)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkMXsMRupHzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_hidden(dim):\n",
        "        return (torch.zeros(1, 1, dim),\n",
        "                torch.zeros(1, 1, dim))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xzubr5h7nl_1",
        "colab_type": "code",
        "outputId": "0527efeb-2afe-423f-ce1a-6ee9bb36749b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "## Try the simple LSTM by hand (no letter embedding)\n",
        "\n",
        "\n",
        "nn_word_embeddings = nn.Embedding(VOCAB_SIZE, WORD_EMBEDDING_SIZE)\n",
        "lstm_full = nn.LSTM(WORD_EMBEDDING_SIZE, HIDDEN_DIM_FULL_LSTM)  \n",
        "hidden_full = init_hidden(HIDDEN_DIM_FULL_LSTM)\n",
        "\n",
        "sentence = 'the dog ate the apple'.split()\n",
        "n_words = len(sentence)\n",
        "word_ids = prepare_sequence(sentence,word_id)\n",
        "print(word_ids)\n",
        "word_embeddings = nn_word_embeddings(word_ids)\n",
        "print(word_embeddings)\n",
        "\n",
        "\n",
        "for word_embedding in word_embeddings:\n",
        "    print(word_embedding.dtype)\n",
        "    # Step through the sequence one element at a time.\n",
        "    # after each step, hidden contains the hidden state.\n",
        "    out, hidden_full = lstm_full(word_embedding.view(1, 1, -1), hidden_full)\n",
        "\n",
        "print(out)\n",
        "# out is our final embedding\n",
        "print(hidden_full)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 2, 0, 3])\n",
            "tensor([[-0.5517,  0.8091, -0.4110,  0.5568,  0.5924,  0.4504],\n",
            "        [ 1.0375,  0.0895, -1.9281,  0.4494,  0.8918,  0.7742],\n",
            "        [-0.9697, -0.6761,  0.7772,  0.6192,  1.6007, -0.9435],\n",
            "        [-0.5517,  0.8091, -0.4110,  0.5568,  0.5924,  0.4504],\n",
            "        [-1.1967, -0.4981, -0.4123,  0.6369,  0.9843, -1.1036]],\n",
            "       grad_fn=<EmbeddingBackward>)\n",
            "torch.float32\n",
            "torch.float32\n",
            "torch.float32\n",
            "torch.float32\n",
            "torch.float32\n",
            "tensor([[[ 0.1472,  0.3769, -0.4417,  0.3877,  0.1941,  0.0209]]],\n",
            "       grad_fn=<StackBackward>)\n",
            "(tensor([[[ 0.1472,  0.3769, -0.4417,  0.3877,  0.1941,  0.0209]]],\n",
            "       grad_fn=<StackBackward>), tensor([[[ 0.2729,  0.6183, -0.6431,  0.7131,  0.2420,  0.0385]]],\n",
            "       grad_fn=<StackBackward>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlW6cC9wqqzg",
        "colab_type": "code",
        "outputId": "8d95673c-0eff-419f-9282-4e7cf94414cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "## Try the letter LSTM by hand\n",
        "\n",
        "\n",
        "\n",
        "lstm_letters = nn.LSTM(N_LETTERS, HIDDEN_DIM_LETTER_LSTM)  \n",
        "hidden_letters = init_hidden(HIDDEN_DIM_LETTER_LSTM)\n",
        "\n",
        "word = 'orange'\n",
        "n_letters = len(word)\n",
        "letter_ids = get_word_vector(word).type(torch.FloatTensor)\n",
        "print(letter_ids)\n",
        "\n",
        "\n",
        "for letter in letter_ids:\n",
        "    # Step through the sequence one element at a time.\n",
        "    # after each step, hidden contains the hidden state.\n",
        "    letter_out, hidden_letters = lstm_letters(letter.view(1, 1, -1), hidden_letters)\n",
        "\n",
        "print(letter_out)\n",
        "# letter_out is our letter level embedding for the word\n",
        "print(hidden_letters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "tensor([[[-0.3223, -0.3033,  0.0765, -0.1239,  0.2630]]],\n",
            "       grad_fn=<StackBackward>)\n",
            "(tensor([[[-0.3223, -0.3033,  0.0765, -0.1239,  0.2630]]],\n",
            "       grad_fn=<StackBackward>), tensor([[[-0.6339, -0.5351,  0.1922, -0.2950,  0.4478]]],\n",
            "       grad_fn=<StackBackward>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79WIt48ytHow",
        "colab_type": "code",
        "outputId": "1f355d78-9c7e-4410-c996-e8457b542d29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "## Putting it all together\n",
        "\n",
        "\n",
        "\n",
        "lstm_letters = nn.LSTM(N_LETTERS, HIDDEN_DIM_LETTER_LSTM)  \n",
        "hidden_letters = init_hidden(HIDDEN_DIM_LETTER_LSTM)\n",
        "\n",
        "nn_word_embeddings = nn.Embedding(VOCAB_SIZE, WORD_EMBEDDING_SIZE)\n",
        "# the full LSTM now takes a larger input : concatenating the original word embedding + letter level embedding (output of first LSTM)\n",
        "lstm_full = nn.LSTM(WORD_EMBEDDING_SIZE+HIDDEN_DIM_LETTER_LSTM, HIDDEN_DIM_FULL_LSTM)  \n",
        "hidden_full = init_hidden(HIDDEN_DIM_FULL_LSTM)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sentence = 'the dog ate the apple'.split()\n",
        "n_words = len(sentence)\n",
        "word_ids = prepare_sequence(sentence,word_id)\n",
        "print(word_ids)\n",
        "word_embeddings = nn_word_embeddings(word_ids)\n",
        "print(word_embeddings)\n",
        "\n",
        "\n",
        "for pos in range(n_words):\n",
        "    word_embedding = word_embeddings[pos]\n",
        "    print(word_embedding.dtype)\n",
        "    \n",
        "    word = sentence[pos]\n",
        "    n_letters = len(word)\n",
        "    letter_ids = get_word_vector(word).type(torch.FloatTensor)\n",
        "\n",
        "    for letter in letter_ids:\n",
        "        letter_out, hidden_letters = lstm_letters(letter.view(1, 1, -1), hidden_letters)\n",
        "\n",
        "    # we prepare to concatenate the word embedding and letter level embedding\n",
        "    letter_out = letter_out.view(-1,1)\n",
        "    word_embedding = word_embedding.view(-1,1)\n",
        "    full_lstm_input = torch.cat([word_embedding,letter_out])\n",
        "\n",
        "    # and can then feed this into the full LSTM\n",
        "    out, hidden_full = lstm_full(full_lstm_input.view(1, 1, -1), hidden_full)\n",
        "\n",
        "\n",
        "print(out)\n",
        "# out is our final embedding\n",
        "print(hidden_full)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 2, 0, 3])\n",
            "tensor([[ 1.4079, -1.4227,  0.3248, -0.8228, -1.6402,  1.6462],\n",
            "        [ 0.6114, -0.5006,  0.2833, -0.2367,  0.6893,  0.4439],\n",
            "        [ 0.9462,  0.3306, -0.8888,  0.3297,  0.1003,  0.0107],\n",
            "        [ 1.4079, -1.4227,  0.3248, -0.8228, -1.6402,  1.6462],\n",
            "        [ 0.3614, -0.8425,  1.0932, -0.3953,  0.7582, -0.1898]],\n",
            "       grad_fn=<EmbeddingBackward>)\n",
            "torch.float32\n",
            "torch.float32\n",
            "torch.float32\n",
            "torch.float32\n",
            "torch.float32\n",
            "tensor([[[ 0.0727, -0.0146,  0.0160,  0.2073,  0.0431,  0.4561]]],\n",
            "       grad_fn=<StackBackward>)\n",
            "(tensor([[[ 0.0727, -0.0146,  0.0160,  0.2073,  0.0431,  0.4561]]],\n",
            "       grad_fn=<StackBackward>), tensor([[[ 0.1238, -0.0183,  0.0897,  0.4026,  0.0818,  0.7613]]],\n",
            "       grad_fn=<StackBackward>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMQXJhfFDRAJ",
        "colab_type": "text"
      },
      "source": [
        "Create the model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxLqCM84DRAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharLevelLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CharLevelLSTM, self).__init__()\n",
        "\n",
        "        self.lstm_letters = nn.LSTM(N_LETTERS, HIDDEN_DIM_LETTER_LSTM)  \n",
        "        self.hidden_letters = init_hidden(HIDDEN_DIM_LETTER_LSTM)\n",
        "\n",
        "        self.nn_word_embeddings = nn.Embedding(VOCAB_SIZE, WORD_EMBEDDING_SIZE)\n",
        "        # the full LSTM now takes a larger input : concatenating the original word embedding + letter level embedding (output of first LSTM)\n",
        "        self.lstm_full = nn.LSTM(WORD_EMBEDDING_SIZE+HIDDEN_DIM_LETTER_LSTM, HIDDEN_DIM_FULL_LSTM)  \n",
        "        self.hidden_full = self.init_hidden(HIDDEN_DIM_FULL_LSTM)\n",
        "\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = nn.Linear(HIDDEN_DIM_FULL_LSTM, TAGSET_SIZE)\n",
        "        \n",
        "\n",
        "    def forward(self, sentence):\n",
        "        # note the input is now actual words, not word_ids\n",
        "\n",
        "        n_words = len(sentence)\n",
        "\n",
        "        word_ids = prepare_sequence(sentence,word_id)\n",
        "        # print(word_ids)\n",
        "        word_embeddings = self.nn_word_embeddings(word_ids)\n",
        "        # print(word_embeddings)\n",
        "\n",
        "        out_cat = torch.zeros(0)\n",
        "        for pos in range(n_words):\n",
        "            word_embedding = word_embeddings[pos]\n",
        "            # print(word_embedding.dtype)\n",
        "            \n",
        "            word = sentence[pos]\n",
        "            n_letters = len(word)\n",
        "            letter_ids = get_word_vector(word).type(torch.FloatTensor)\n",
        "\n",
        "            for letter in letter_ids:\n",
        "                letter_out, self.hidden_letters = self.lstm_letters(letter.view(1, 1, -1), self.hidden_letters)\n",
        "\n",
        "            # we prepare to concatenate the word embedding and letter level embedding\n",
        "            letter_out = letter_out.view(-1,1)\n",
        "            word_embedding = word_embedding.view(-1,1)\n",
        "            full_lstm_input = torch.cat([word_embedding,letter_out])\n",
        "\n",
        "            # and can then feed this into the full LSTM\n",
        "            out, self.hidden_full = self.lstm_full(full_lstm_input.view(1, 1, -1), self.hidden_full)\n",
        "            out_cat = torch.cat([out_cat, out[-1]], dim=0)\n",
        "\n",
        "        # print(out)\n",
        "        # out is our final embedding\n",
        "        # print(hidden_full)\n",
        "\n",
        "        tag_space = self.hidden2tag(out_cat.view(n_words, -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores\n",
        "    \n",
        "    \n",
        "    def init_hidden(self,dim):\n",
        "        return (torch.zeros(1, 1, dim),torch.zeros(1, 1, dim))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_xLRqENxUmX",
        "colab_type": "code",
        "outputId": "e80e8f0e-07fe-451e-b41c-9672f4e539a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "training_data[0][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', 'dog', 'ate', 'the', 'apple']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQL824RODRAT",
        "colab_type": "text"
      },
      "source": [
        "Train the model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJD0omS4DRAV",
        "colab_type": "code",
        "outputId": "cc7af75c-33ba-4e4b-88ee-605bcc580829",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "model = CharLevelLSTM()\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# See what the scores are before training\n",
        "# Note that element i,j of the output is the score for tag j for word i.\n",
        "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
        "with torch.no_grad():\n",
        "    tag_scores = model(training_data[0][0])\n",
        "    print(tag_scores)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.2832, -1.0096, -1.0258],\n",
            "        [-1.2158, -1.0594, -1.0304],\n",
            "        [-1.3032, -1.0789, -0.9458],\n",
            "        [-1.3074, -1.0411, -0.9771],\n",
            "        [-1.3222, -1.0270, -0.9798]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbZjV3L7B3Qg",
        "colab_type": "code",
        "outputId": "ebe5611e-d824-4a6e-d3bd-e2a4d0298148",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "\n",
        "for epoch in range(5):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "    for sentence, tags in training_data:\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Also, we need to clear out the hidden state of the LSTM,\n",
        "        # detaching it from its history on the last instance.\n",
        "        #model.hidden = model.init_hidden()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
        "        # Tensors of word indices.\n",
        "        #sentence_in = prepare_sequence(sentence, word_id)\n",
        "        targets = prepare_sequence(tags, tag_id)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        tag_scores = model(sentence)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        #  calling optimizer.step()\n",
        "        loss = loss_function(tag_scores,targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# See what the scores are after training\n",
        "with torch.no_grad():\n",
        "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    tag_scores = model(inputs)\n",
        "\n",
        "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
        "    # for word i. The predicted tag is the maximum scoring tag.\n",
        "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
        "    # since 0 is index of the maximum value of row 1,\n",
        "    # 1 is the index of maximum value of row 2, etc.\n",
        "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
        "    print(tag_scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-8a15daaa90fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#  calling optimizer.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [6, 24]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPa8N6eTDRAf",
        "colab_type": "text"
      },
      "source": [
        "Exercise: Augmenting the LSTM part-of-speech tagger with character-level features\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "In the example above, each word had an embedding, which served as the\n",
        "inputs to our sequence model. Let's augment the word embeddings with a\n",
        "representation derived from the characters of the word. We expect that\n",
        "this should help significantly, since character-level information like\n",
        "affixes have a large bearing on part-of-speech. For example, words with\n",
        "the affix *-ly* are almost always tagged as adverbs in English.\n",
        "\n",
        "To do this, let $c_w$ be the character-level representation of\n",
        "word $w$. Let $x_w$ be the word embedding as before. Then\n",
        "the input to our sequence model is the concatenation of $x_w$ and\n",
        "$c_w$. So if $x_w$ has dimension 5, and $c_w$\n",
        "dimension 3, then our LSTM should accept an input of dimension 8.\n",
        "\n",
        "To get the character level representation, do an LSTM over the\n",
        "characters of a word, and let $c_w$ be the final hidden state of\n",
        "this LSTM. Hints:\n",
        "\n",
        "* There are going to be two LSTM's in your new model.\n",
        "  The original one that outputs POS tag scores, and the new one that\n",
        "  outputs a character-level representation of each word.\n",
        "* To do a sequence model over characters, you will have to embed characters.\n",
        "  The character embeddings will be the input to the character LSTM.\n",
        "\n",
        "\n"
      ]
    }
  ]
}