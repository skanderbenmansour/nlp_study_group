## Experiment Ideas

### Hyperparameter tuning
--> batch size

--> hidden dimension

--> number of layers

--> number of epochs


### Preprocessing
--> Unfreeze glove embeddings and continue training

--> Train new embeddings

--> Different word embeddings (word2vec,fasttext,paranmt)

--> Effect of lemmatization/puncation removal


### Other model types
(probably easiest to start with batch size=1 to debug)

--> Bi-LSTM

--> Add attention layer

--> GRU

--> CNN
