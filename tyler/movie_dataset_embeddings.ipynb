{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "source is https://www.kaggle.com/c/word2vec-nlp-tutorial/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/tyler/Documents/programming/pytorch_nlp/data/word2vec-nlp-tutorial/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path+'labeledTrainData.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[:15000]\n",
    "val = df[15000:20000]\n",
    "test = df[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 5000, 5000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train),len(val),len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_review(review):\n",
    "    chars = ['/','\\\\','>','<','-','br']\n",
    "    chars.extend('1 2 3 4 5 6 7 8 9 0'.split())\n",
    "    for char in chars:\n",
    "        review = review.replace(char,'')\n",
    "    \n",
    "    tokens = word_tokenize(review)\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make vocab and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tyler/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb5c53d77434c6bb26cc17b6f5c9bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labels = list(train.sentiment)\n",
    "reviews = list(train.review.values)\n",
    "\n",
    "all_words = [process_review(review) for review in tqdm(reviews)]\n",
    "\n",
    "train_data = list(zip(all_words,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in all_words for item in sublist]\n",
    "vocab = set(flat_list)\n",
    "\n",
    "len(vocab)\n",
    "\n",
    "word_to_idx = {word:idx for idx,word in enumerate(list(vocab))}\n",
    "\n",
    "counts = Counter(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(flat_list).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 10\n",
    "keep = counts[start:20000+start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 52046), ('this', 45732), ('that', 44178), (\"'s\", 37794), ('was', 30368)]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [word for word,count in keep]\n",
    "vocab.append('UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {word:idx for idx,word in enumerate(list(vocab))}\n",
    "idx_to_word = {idx:word for word,idx in word_to_idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed10e17bec79478880668f5294ba4166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labels = list(test.sentiment)\n",
    "reviews = list(test.review.values)\n",
    "\n",
    "all_words = [process_review(review) for review in tqdm(reviews)]\n",
    "\n",
    "test_data = list(zip(all_words,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa70b8c11edc4e0cbffd129bd821aa65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labels = list(val.sentiment)\n",
    "reviews = list(val.review.values)\n",
    "\n",
    "all_words = [process_review(review) for review in tqdm(reviews)]\n",
    "\n",
    "val_data = list(zip(all_words,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW --> Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5813bae96284501b0d4340b54f00eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for sentence, label in tqdm(train_data):\n",
    "    for i in range(2, len(sentence) - 2):\n",
    "        context = [sentence[i - 2], sentence[i - 1],sentence[i + 1], sentence[i + 2]]\n",
    "        target = sentence[i]\n",
    "        data.append((context, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_data = []\n",
    "\n",
    "for context,target_word in data:\n",
    "    inputs = make_context_vector(context, word_to_idx)\n",
    "    target = to_idx(target_word)\n",
    "    tensor_data.append((inputs,target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(tensor_data, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size,batch_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        embeds_sum = embeds.sum(dim=1)\n",
    "        out = F.relu(self.linear1(embeds_sum))\n",
    "        out = self.linear2(out)\n",
    "        out = out.view(batch_size,-1)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_idx(w):\n",
    "    idx = word_to_idx.get(w)\n",
    "    if idx is None:\n",
    "        idx = 20000\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context_vector(context, word_to_idx):\n",
    "    \n",
    "    idxs = [to_idx(w) for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab) + 1\n",
    "embedding_dim = 10\n",
    "context_size = 4\n",
    "batch_size = 1000\n",
    "cbow = CBOW(vocab_size, embedding_dim, context_size,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(cbow.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: train loss of 8.339\n",
      "Epoch 2/10: train loss of 8.000\n",
      "Epoch 3/10: train loss of 8.044\n",
      "Epoch 4/10: train loss of 7.909\n",
      "Epoch 5/10: train loss of 7.744\n",
      "Epoch 6/10: train loss of 7.975\n",
      "Epoch 7/10: train loss of 7.644\n",
      "Epoch 8/10: train loss of 7.706\n",
      "Epoch 9/10: train loss of 7.805\n",
      "Epoch 10/10: train loss of 7.698\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    idx = 0\n",
    "    for context,target_word in data_loader:\n",
    "        if idx  < 200:\n",
    "            #inputs = make_context_vector(context, word_to_idx)\n",
    "            #target = torch.tensor([word_to_idx[target_word]], dtype=torch.long)\n",
    "            inputs = context\n",
    "            target = target_word\n",
    "            cbow.zero_grad()\n",
    "\n",
    "            log_probs = cbow(inputs)\n",
    "            loss = loss_function(log_probs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            idx += 1\n",
    "\n",
    "    train_loss.append(loss.item())\n",
    "    mean_train_loss = np.mean(train_loss)\n",
    "    losses.append(round(mean_train_loss,2))\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}: train loss of {mean_train_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting UNK [get] married ... ==> UNK\n",
      "UNK worst [UNK] all UNK ==> UNK\n",
      "irish filmmakers [kim] UNK UNK ==> UNK\n",
      "really UNK [UNK] UNK me ==> UNK\n",
      "forced UNK [UNK] 's an ==> UNK\n",
      "UNK UNK [greek] tragedy UNK ==> UNK\n",
      "horror movie [UNK] supposed UNK ==> UNK\n",
      "( UNK [UNK] climax UNK ==> UNK\n",
      "UNK annoying [UNK] UNK bit ==> this\n",
      "tv show [UNK] i wo ==> UNK\n",
      "interesting drama [UNK] considering UNK ==> UNK\n",
      "UNK rather [bad] UNK though ==> UNK\n",
      "again that [he] UNK absolutely ==> UNK\n",
      "film UNK [you] can assume ==> UNK\n",
      "suspects ) [UNK] found their ==> UNK\n",
      "UNK despicable [judge] who allows ==> UNK\n",
      "UNK london [UNK] she UNK ==> UNK\n",
      "arjun does [reach] boiling point ==> UNK\n",
      "but now [that] i 've ==> UNK\n",
      "transformed from [being] that romantic ==> UNK\n"
     ]
    }
   ],
   "source": [
    "for context,target_word in data_loader:\n",
    "    inputs = context\n",
    "    target = target_word\n",
    "    cbow.eval()\n",
    "    log_probs = cbow(inputs)\n",
    "    argmax = log_probs.argmax(dim=1)\n",
    "    \n",
    "    for i in range(20):\n",
    "        c = context[i]\n",
    "        c1 = idx_to_word[int(c[0])]\n",
    "        c2 = idx_to_word[int(c[1])]\n",
    "        c3 = idx_to_word[int(c[2])]\n",
    "        c4 = idx_to_word[int(c[3])]\n",
    "        prediction = idx_to_word[int(argmax[i])]\n",
    "        t = idx_to_word[int(target[i])]\n",
    "        print(f'{c1} {c2} [{t}] {c3} {c4} ==> {prediction}')\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'model_checkpoints/embeddings.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cbow.state_dict(),save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = cbow.embeddings.weight.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings --> Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target(label, label_to_idx):\n",
    "    return torch.LongTensor([label_to_idx[label]])\n",
    "\n",
    "def make_bow_vector(sentence, word_to_idx):\n",
    "    vec = torch.zeros(len(word_to_idx),dtype=torch.long)\n",
    "    for word in sentence:\n",
    "        if word in word_to_idx:\n",
    "            vec[word_to_idx[word]] += 1\n",
    "    return vec.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class embed_classifier(nn.Module):\n",
    "    def __init__(self, num_labels, vocab_size, embedding_dim,hidden):\n",
    "        super(embed_classifier, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, hidden)\n",
    "        self.linear_2 = nn.Linear(hidden, num_labels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        embeds_sum = embeds.sum(dim=1)\n",
    "        x = self.linear(embeds_sum)\n",
    "        #rint(x.shape)\n",
    "        x = self.linear_2(x)\n",
    "        #print(x.shape)\n",
    "        return F.log_softmax((x), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_to_idx) + 1\n",
    "NUM_LABELS = 2\n",
    "embedding_dim = 10\n",
    "hidden = 10\n",
    "model = embed_classifier(NUM_LABELS, VOCAB_SIZE,embedding_dim,hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20001"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embeddings = nn.Embedding.weight.data.copy_(torch.tensor(weights))\n",
    "model.embeddings.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "lr = .001\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2718.1111,     0.0000]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    sample = all_words[4]\n",
    "    bow_vector = make_bow_vector(sample[:20], word_to_idx)\n",
    "    log_probs = model(bow_vector)\n",
    "    print(log_probs)\n",
    "    loss = -loss_function(log_probs, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: train loss of 1.122, val loss of 0.695\n",
      "Epoch 2/10: train loss of 1.101, val loss of 0.696\n",
      "Epoch 3/10: train loss of 1.125, val loss of 0.699\n",
      "Epoch 4/10: train loss of 1.090, val loss of 0.697\n",
      "Epoch 5/10: train loss of 1.114, val loss of 0.700\n",
      "Epoch 6/10: train loss of 1.086, val loss of 0.714\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-392-930ae1b9b09e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#print(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_path = 'model_checkpoints/embed_classifier.pt'\n",
    "val_loss_min = np.Inf\n",
    "val_loss_min = 0.694\n",
    "num_epochs = 10\n",
    "loss_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for sentence, label in train_data:\n",
    "        model.zero_grad()\n",
    "\n",
    "        vec = make_bow_vector(sentence, word_to_idx)\n",
    "        vec = torch.LongTensor(vec)\n",
    "        target = torch.LongTensor([label])\n",
    "\n",
    "        log_probs = model(vec)\n",
    "\n",
    "        loss = loss_function(log_probs, target)\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    mean_train_loss = np.mean(train_loss)\n",
    "    val_loss = []\n",
    "    for sentence, label in val_data:\n",
    "        model.eval()\n",
    "\n",
    "        vec = make_bow_vector(sentence, word_to_idx)\n",
    "        target = torch.LongTensor([label])\n",
    "\n",
    "        log_probs = model(vec)\n",
    "        pred = log_probs.argmax().detach().numpy()\n",
    "        loss = loss_function(log_probs, target)\n",
    "\n",
    "        val_loss.append(loss.item())\n",
    "        \n",
    "    mean_val_loss = np.mean(val_loss)\n",
    "    \n",
    "    loss_history.append((mean_train_loss,mean_val_loss))\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}: train loss of {mean_train_loss:.3f}, val loss of {mean_val_loss:.3f}')\n",
    "    \n",
    "    if mean_val_loss <= val_loss_min:\n",
    "        print(f'Val loss decreased {val_loss_min:.3f} --> {mean_val_loss:.3f} saving model...')\n",
    "        torch.save(model.state_dict(),save_path)\n",
    "        val_loss_min = mean_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----TRAIN SET----\n",
      "train loss of 0.695\n",
      "train accuracy of 50.09\n",
      "----VAL SET----\n",
      "val loss of 0.695\n",
      "val accuracy of 50.3\n",
      "----TEST SET----\n",
      "test loss of 0.696\n",
      "test accuracy of 49.44\n"
     ]
    }
   ],
   "source": [
    "names = 'train val test'.split()\n",
    "data_list = [train_data,val_data,test_data]\n",
    "\n",
    "for name,data in zip(names,data_list):\n",
    "    eval_loss = []\n",
    "    num_correct = 0\n",
    "    to_eval = test_data\n",
    "    for sentence, label in data:\n",
    "        model.eval()\n",
    "\n",
    "        vec = make_bow_vector(sentence, word_to_idx)\n",
    "        target = torch.LongTensor([label])\n",
    "\n",
    "        log_probs = model(vec)\n",
    "        pred = log_probs.argmax().detach().numpy()\n",
    "        correct = int(pred == label)\n",
    "        num_correct += correct\n",
    "        loss = loss_function(log_probs, target)\n",
    "\n",
    "        eval_loss.append(loss.item())\n",
    "    \n",
    "    mean_loss = np.mean(eval_loss)\n",
    "    print(f'----{name} set----'.upper())\n",
    "    print(f'{name} loss of {round(mean_loss,3)}')\n",
    "    print(f'{name} accuracy of {round(num_correct*100/len(data),2)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pretained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "class glove_classifier(nn.Module):\n",
    "    def __init__(self, num_labels, vocab_size, embedding_dim,hidden,glove):\n",
    "        super(glove_classifier, self).__init__()\n",
    "        self.embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(glove.vectors))\n",
    "        self.embeddings.requires_grad = False\n",
    "        self.linear = nn.Linear(embedding_dim, hidden)\n",
    "        self.linear_2 = nn.Linear(hidden, num_labels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        embeds_sum = embeds.sum(dim=1)\n",
    "        x = self.linear(embeds_sum)\n",
    "        #rint(x.shape)\n",
    "        x = self.linear_2(x)\n",
    "        #print(x.shape)\n",
    "        return F.softmax((x), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/tyler/Documents/programming/embeddings/models/glove.840B.300d.model'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = KeyedVectors.load_word2vec_format(path,limit=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.FloatTensor(glove.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = 2\n",
    "VOCAB_SIZE = 100000\n",
    "embedding_dim = 300\n",
    "hidden = 100\n",
    "\n",
    "model = glove_classifier(NUM_LABELS, VOCAB_SIZE,embedding_dim,hidden,glove)\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "lr = .005\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.8155)"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([0,1],dtype=torch.float)\n",
    "b = torch.tensor([.5,.5],dtype=torch.float)\n",
    "loss_function(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100000, 300])"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of'"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.index2word[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word:idx for idx,word in enumerate(glove.vocab.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2534"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['episode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input(sentence, word2idx):\n",
    "    vec = torch.zeros(len(word2idx),dtype=torch.long)\n",
    "    for word in sentence:\n",
    "        if word in word2idx:\n",
    "            vec[word2idx[word]] = 1\n",
    "    return vec.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ', , this is a sentence asdf'.split()\n",
    "x = make_input(sentence, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.tensor([0,1],dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    sample = all_words[4]\n",
    "    vec = make_input(sample[:20], word_to_idx)\n",
    "    log_probs = model(vec)\n",
    "    print(log_probs)\n",
    "    loss = loss_function(log_probs, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: train loss of 13.318, val loss of 14.092\n",
      "Val loss decreased inf --> 14.092 saving model...\n",
      "Epoch 2/10: train loss of 13.318, val loss of 14.092\n",
      "Val loss decreased 14.092 --> 14.092 saving model...\n",
      "Epoch 3/10: train loss of 13.318, val loss of 14.092\n",
      "Val loss decreased 14.092 --> 14.092 saving model...\n",
      "Epoch 4/10: train loss of 13.318, val loss of 14.092\n",
      "Val loss decreased 14.092 --> 14.092 saving model...\n",
      "Epoch 5/10: train loss of 13.318, val loss of 14.092\n",
      "Val loss decreased 14.092 --> 14.092 saving model...\n",
      "Epoch 6/10: train loss of 13.318, val loss of 14.092\n",
      "Val loss decreased 14.092 --> 14.092 saving model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-528-6850867e9a9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#target = torch.LongTensor([label])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-508-66100c9d20ba>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0membeds_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#rint(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_path = 'model_checkpoints/glove_classifier.pt'\n",
    "val_loss_min = np.Inf\n",
    "#val_loss_min = 0.694\n",
    "num_epochs = 10\n",
    "loss_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = []\n",
    "    for sentence, label in train_data[:1000]:\n",
    "        model.zero_grad()\n",
    "\n",
    "        vec = make_input(sentence, word2idx)\n",
    "        vec = torch.LongTensor(vec)\n",
    "        if label == 0:\n",
    "            target = torch.tensor([1,0],dtype=torch.float)\n",
    "        else:\n",
    "            target = torch.tensor([0,1],dtype=torch.float)\n",
    "        #target = torch.LongTensor([label])\n",
    "\n",
    "        log_probs = model(vec)\n",
    "\n",
    "        loss = loss_function(log_probs, target)\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    mean_train_loss = np.mean(train_loss)\n",
    "    val_loss = []\n",
    "    for sentence, label in val_data[:100]:\n",
    "        model.eval()\n",
    "\n",
    "        vec = make_input(sentence, word2idx)\n",
    "        if label == 0:\n",
    "            target = torch.tensor([1,0],dtype=torch.float)\n",
    "        else:\n",
    "            target = torch.tensor([0,1],dtype=torch.float)\n",
    "        #target = torch.LongTensor([label])\n",
    "\n",
    "        log_probs = model(vec)\n",
    "        pred = log_probs.argmax().detach().numpy()\n",
    "        loss = loss_function(log_probs, target)\n",
    "\n",
    "        val_loss.append(loss.item())\n",
    "        \n",
    "    mean_val_loss = np.mean(val_loss)\n",
    "    \n",
    "    loss_history.append((mean_train_loss,mean_val_loss))\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}: train loss of {mean_train_loss:.3f}, val loss of {mean_val_loss:.3f}')\n",
    "    \n",
    "    if mean_val_loss <= val_loss_min:\n",
    "        print(f'Val loss decreased {val_loss_min:.3f} --> {mean_val_loss:.3f} saving model...')\n",
    "        torch.save(model.state_dict(),save_path)\n",
    "        val_loss_min = mean_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.539200534820557"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0)"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----TRAIN SET----\n",
      "train loss of 4116.119\n",
      "train accuracy of 55.0\n",
      "----VAL SET----\n",
      "val loss of 4664.869\n",
      "val accuracy of 49.0\n",
      "----TEST SET----\n",
      "test loss of 4756.277\n",
      "test accuracy of 48.0\n"
     ]
    }
   ],
   "source": [
    "names = 'train val test'.split()\n",
    "num = 100\n",
    "data_list = [train_data[:num],val_data[:num],test_data[:num]]\n",
    "\n",
    "for name,data in zip(names,data_list):\n",
    "    eval_loss = []\n",
    "    num_correct = 0\n",
    "    to_eval = test_data\n",
    "    for sentence, label in data:\n",
    "        model.eval()\n",
    "\n",
    "        vec = make_input(sentence, word2idx)\n",
    "        target = torch.LongTensor([label])\n",
    "\n",
    "        log_probs = model(vec)\n",
    "        pred = log_probs.argmax().detach().numpy()\n",
    "        correct = int(pred == label)\n",
    "        num_correct += correct\n",
    "        loss = loss_function(log_probs, target)\n",
    "\n",
    "        eval_loss.append(loss.item())\n",
    "    \n",
    "    mean_loss = np.mean(eval_loss)\n",
    "    print(f'----{name} set----'.upper())\n",
    "    print(f'{name} loss of {round(mean_loss,3)}')\n",
    "    print(f'{name} accuracy of {round(num_correct*100/len(data),2)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9665"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.index((sentence, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
