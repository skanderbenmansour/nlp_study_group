{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"name":"LSTM_char_level.ipynb","provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/sequence_models_tutorial.ipynb","timestamp":1588270333019}],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"P8X5BSymDQ_w","colab_type":"code","outputId":"35b9aa74-f020-4981-cf2b-473768c2815c","executionInfo":{"status":"ok","timestamp":1588318405493,"user_tz":-60,"elapsed":2859,"user":{"displayName":"Skander Ben Mansour","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl7domjq2nPtW4SYKR5BEYuj4qkl2Ljge9t-vlBiU=s64","userId":"11506237701449758837"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import string\n","from collections import defaultdict\n","import numpy as np\n","\n","torch.manual_seed(1)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f0531ff3270>"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"64VoojxzDQ_5","colab_type":"code","outputId":"6550080b-ab81-4b64-a577-e58ae2724b90","executionInfo":{"status":"ok","timestamp":1588318405497,"user_tz":-60,"elapsed":1036,"user":{"displayName":"Skander Ben Mansour","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl7domjq2nPtW4SYKR5BEYuj4qkl2Ljge9t-vlBiU=s64","userId":"11506237701449758837"}},"colab":{"base_uri":"https://localhost:8080/","height":571}},"source":["# SENTENCE LSTM EXAMPLE\n","\n","\n","# We imagine we have a  word embedding space of space of dim 10 and the LSTM returns an embedding of dim 5\n","lstm = nn.LSTM(10, 5) \n","inputs = [torch.randn(1, 10) for _ in range(5)]  # make a sentence of length 5\n","print(inputs)\n","\n","# initialize the hidden state.\n","hidden = (torch.randn(1, 1, 5),\n","          torch.randn(1, 1, 5))\n","\n","for i in inputs:\n","    # Step through the sequence one element at a time.\n","    # after each step, hidden contains the hidden state.\n","    out, hidden = lstm(i.view(1, 1, -1), hidden)\n","\n","## or all at once\n","\n","inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n","print(inputs)\n","# clean out hidden state like above\n","hidden = (torch.randn(1, 1, 5),\n","          torch.randn(1, 1, 5))  \n","\n","out, hidden = lstm(inputs, hidden)\n","print(out)\n","print(hidden)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[tensor([[ 1.4697, -0.3951, -0.5101,  1.1163, -0.5926,  0.9089, -1.0792, -0.6751,\n","          1.4083, -1.8456]]), tensor([[-0.5655, -0.9094, -0.4891, -0.6175,  0.3836,  0.3852,  0.7890,  1.1363,\n","         -0.1147, -0.0180]]), tensor([[ 0.0491,  0.4272, -0.8996,  0.5313,  0.4034,  1.4521, -2.4182, -1.1906,\n","          0.6964,  1.1296]]), tensor([[ 0.2214, -0.0558,  1.2057,  1.9486, -0.0766, -0.8562, -0.7870, -0.8161,\n","          0.5470, -1.1707]]), tensor([[-0.4699, -1.6271, -0.1127,  1.5980, -0.8445, -1.0489,  0.9387,  0.5378,\n","          1.5372, -0.6943]])]\n","tensor([[[ 1.4697, -0.3951, -0.5101,  1.1163, -0.5926,  0.9089, -1.0792,\n","          -0.6751,  1.4083, -1.8456]],\n","\n","        [[-0.5655, -0.9094, -0.4891, -0.6175,  0.3836,  0.3852,  0.7890,\n","           1.1363, -0.1147, -0.0180]],\n","\n","        [[ 0.0491,  0.4272, -0.8996,  0.5313,  0.4034,  1.4521, -2.4182,\n","          -1.1906,  0.6964,  1.1296]],\n","\n","        [[ 0.2214, -0.0558,  1.2057,  1.9486, -0.0766, -0.8562, -0.7870,\n","          -0.8161,  0.5470, -1.1707]],\n","\n","        [[-0.4699, -1.6271, -0.1127,  1.5980, -0.8445, -1.0489,  0.9387,\n","           0.5378,  1.5372, -0.6943]]])\n","tensor([[[ 0.5595, -0.0880, -0.1437, -0.4697,  0.2218]],\n","\n","        [[ 0.0775, -0.0024, -0.3386, -0.2280,  0.3244]],\n","\n","        [[-0.0792, -0.1593, -0.0167, -0.3246,  0.0927]],\n","\n","        [[ 0.1874, -0.2819, -0.0086, -0.0823,  0.0315]],\n","\n","        [[ 0.2854, -0.2004, -0.0710, -0.1411,  0.3178]]],\n","       grad_fn=<StackBackward>)\n","(tensor([[[ 0.2854, -0.2004, -0.0710, -0.1411,  0.3178]]],\n","       grad_fn=<StackBackward>), tensor([[[ 0.7698, -0.6869, -0.3282, -0.3147,  0.3889]]],\n","       grad_fn=<StackBackward>))\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3yrgMmhqSnKv","colab_type":"code","outputId":"a3feb924-a7d4-4c93-cd8d-4bfcd6e57f43","executionInfo":{"status":"ok","timestamp":1588318476253,"user_tz":-60,"elapsed":492,"user":{"displayName":"Skander Ben Mansour","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl7domjq2nPtW4SYKR5BEYuj4qkl2Ljge9t-vlBiU=s64","userId":"11506237701449758837"}},"colab":{"base_uri":"https://localhost:8080/","height":857}},"source":["# LETTERS LSTM EXAMPLE\n","\n","\n","# letters have an embedding space of dim 26 and the LSTM returns an embedding of dim say 4\n","lstm = nn.LSTM(26, 4) \n","inputs = [torch.randn(1, 26) for _ in range(5)]  # make a word of length 5 letters\n","print(inputs)\n","\n","# initialize the hidden state.\n","hidden = (torch.randn(1, 1, 4),\n","          torch.randn(1, 1, 4))\n","\n","for i in inputs:\n","    # Step through the sequence one element at a time.\n","    # after each step, hidden contains the hidden state.\n","    out, hidden = lstm(i.view(1, 1, -1), hidden)\n","\n","## or all at once\n","\n","inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n","print(inputs)\n","# clean out hidden state like above\n","hidden = (torch.randn(1, 1, 4),\n","          torch.randn(1, 1, 4))  \n","\n","out, hidden = lstm(inputs, hidden)\n","print(out)\n","\n","# the hidden layer will be our character level embedding\n","# it will be concatenated to our word embedding, before being fed into the sentence level LSTM\n","print(hidden)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[tensor([[-0.8574,  1.8176,  1.1826, -1.3298,  0.5813,  0.5272,  1.7288, -0.4040,\n","         -0.8616,  1.7713, -0.5801, -1.8529,  0.7450,  0.6500,  1.9265,  0.7164,\n","         -0.1715, -1.3550, -1.1653,  1.6195,  0.3746, -0.8004, -1.1944,  0.5411,\n","          1.7009, -0.1816]]), tensor([[-1.2272, -0.9012,  0.5480, -2.0712,  0.4554, -1.2638,  0.2165,  0.7955,\n","         -1.1174, -0.5562,  0.7357, -0.0418, -0.7948, -1.4730,  1.8940,  0.1557,\n","          0.6981,  0.7614,  2.2613, -0.4493, -0.1287, -0.4660,  1.7010, -1.4658,\n","         -0.1103,  1.0926]]), tensor([[-0.5007,  0.1164, -0.7695,  2.2969, -0.2332,  0.4124, -1.2673,  1.5329,\n","          1.4025,  0.8013, -1.3451, -0.9675, -1.8558,  0.3900,  0.8584,  0.6327,\n","         -0.0534,  0.1181, -0.8883, -0.3490, -0.4198,  1.0553, -0.6366,  1.6225,\n","          0.2911,  0.0630]]), tensor([[ 0.0054,  0.1899, -0.2596, -1.7663,  0.8808, -1.4067,  2.3293,  0.1724,\n","         -0.0635, -1.3799,  0.3122, -1.0149,  0.0527, -0.5512,  1.1984,  0.2158,\n","          0.3646,  0.2221,  0.1389, -2.3915,  1.2747, -1.5222,  1.3066, -2.3510,\n","         -0.4519, -1.4112]]), tensor([[-0.2825, -0.3865,  0.5055, -0.2368,  1.6691, -0.0307,  0.4252, -0.1734,\n","         -0.2954,  1.7890,  0.4945, -0.3246,  0.5310, -1.6991,  0.8443, -1.0595,\n","          0.5408, -0.2896, -0.2844,  0.8368,  0.3623,  0.0845,  0.3374, -0.4380,\n","          1.0857,  0.2385]])]\n","tensor([[[-0.8574,  1.8176,  1.1826, -1.3298,  0.5813,  0.5272,  1.7288,\n","          -0.4040, -0.8616,  1.7713, -0.5801, -1.8529,  0.7450,  0.6500,\n","           1.9265,  0.7164, -0.1715, -1.3550, -1.1653,  1.6195,  0.3746,\n","          -0.8004, -1.1944,  0.5411,  1.7009, -0.1816]],\n","\n","        [[-1.2272, -0.9012,  0.5480, -2.0712,  0.4554, -1.2638,  0.2165,\n","           0.7955, -1.1174, -0.5562,  0.7357, -0.0418, -0.7948, -1.4730,\n","           1.8940,  0.1557,  0.6981,  0.7614,  2.2613, -0.4493, -0.1287,\n","          -0.4660,  1.7010, -1.4658, -0.1103,  1.0926]],\n","\n","        [[-0.5007,  0.1164, -0.7695,  2.2969, -0.2332,  0.4124, -1.2673,\n","           1.5329,  1.4025,  0.8013, -1.3451, -0.9675, -1.8558,  0.3900,\n","           0.8584,  0.6327, -0.0534,  0.1181, -0.8883, -0.3490, -0.4198,\n","           1.0553, -0.6366,  1.6225,  0.2911,  0.0630]],\n","\n","        [[ 0.0054,  0.1899, -0.2596, -1.7663,  0.8808, -1.4067,  2.3293,\n","           0.1724, -0.0635, -1.3799,  0.3122, -1.0149,  0.0527, -0.5512,\n","           1.1984,  0.2158,  0.3646,  0.2221,  0.1389, -2.3915,  1.2747,\n","          -1.5222,  1.3066, -2.3510, -0.4519, -1.4112]],\n","\n","        [[-0.2825, -0.3865,  0.5055, -0.2368,  1.6691, -0.0307,  0.4252,\n","          -0.1734, -0.2954,  1.7890,  0.4945, -0.3246,  0.5310, -1.6991,\n","           0.8443, -1.0595,  0.5408, -0.2896, -0.2844,  0.8368,  0.3623,\n","           0.0845,  0.3374, -0.4380,  1.0857,  0.2385]]])\n","tensor([[[ 0.0036, -0.2952,  0.1632, -0.3713]],\n","\n","        [[-0.0425, -0.2475,  0.0112, -0.0679]],\n","\n","        [[ 0.0024, -0.0874, -0.0197,  0.1160]],\n","\n","        [[-0.1553,  0.0530,  0.0421,  0.0210]],\n","\n","        [[-0.0379, -0.2707, -0.3295, -0.0112]]], grad_fn=<StackBackward>)\n","(tensor([[[-0.0379, -0.2707, -0.3295, -0.0112]]], grad_fn=<StackBackward>), tensor([[[-0.1395, -0.4627, -0.4121, -0.0875]]], grad_fn=<StackBackward>))\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bDnaPl4RK2Fu","colab_type":"code","outputId":"07175cf9-da3e-405c-a09c-97b7f23b7352","executionInfo":{"status":"ok","timestamp":1588321024222,"user_tz":-60,"elapsed":617,"user":{"displayName":"Skander Ben Mansour","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl7domjq2nPtW4SYKR5BEYuj4qkl2Ljge9t-vlBiU=s64","userId":"11506237701449758837"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["training_data = [\n","    (\"The dog ate the apple\".lower().split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n","    (\"Everybody read that book\".lower().split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n","]\n","\n","letter_id = dict(zip(string.ascii_letters[:26],range(26)))\n","\n","word_id = defaultdict(lambda:len(word_id))\n","all_tags = set()\n","\n","for sent, tags in training_data:\n","    all_tags |= set(tags)\n","    for word in sent:\n","        word_id[word]\n","tag_id = dict(zip(all_tags,range(len(all_tags))))\n","word_id = dict(word_id)\n","\n","print(letter_id)\n","print(word_id)\n","print(tag_id)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25}\n","{'the': 0, 'dog': 1, 'ate': 2, 'apple': 3, 'everybody': 4, 'read': 5, 'that': 6, 'book': 7}\n","{'V': 0, 'NN': 1, 'DET': 2}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MsZymdAKDRAB","colab_type":"code","colab":{}},"source":["\n","\n","def prepare_sequence(seq, to_ix):\n","    idxs = [to_ix[w] for w in seq]\n","    return torch.tensor(idxs, dtype=torch.long)\n","\n","def get_letter_vector(letter):\n","    v = np.zeros(26)\n","    v[letter_id[letter]] = 1\n","    return torch.tensor(v,dtype=torch.long)\n","\n","def get_word_vector(word):\n","  return torch.cat([get_letter_vector(letter) for letter in word]).view(-1,26)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0l_VaRV5N9Ld","colab_type":"code","outputId":"76d836f3-c6a4-4503-aaf4-2098ae2c974e","executionInfo":{"status":"ok","timestamp":1588327479251,"user_tz":-60,"elapsed":861,"user":{"displayName":"Skander Ben Mansour","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl7domjq2nPtW4SYKR5BEYuj4qkl2Ljge9t-vlBiU=s64","userId":"11506237701449758837"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["prepare_sequence('the ate'.split(),word_id)"],"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 2])"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"dcQIiqqgM_dE","colab_type":"code","outputId":"48a8c4f3-b46e-4d0a-ac16-04ad1c4c873e","executionInfo":{"status":"ok","timestamp":1588321035480,"user_tz":-60,"elapsed":633,"user":{"displayName":"Skander Ben Mansour","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl7domjq2nPtW4SYKR5BEYuj4qkl2Ljge9t-vlBiU=s64","userId":"11506237701449758837"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["get_letter_vector('b')"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0])"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"aNl6nK_kNdkW","colab_type":"code","outputId":"bc11b2b2-bdb7-4ed5-cf78-f2572b205573","executionInfo":{"status":"ok","timestamp":1588327482433,"user_tz":-60,"elapsed":581,"user":{"displayName":"Skander Ben Mansour","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl7domjq2nPtW4SYKR5BEYuj4qkl2Ljge9t-vlBiU=s64","userId":"11506237701449758837"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["get_word_vector('ab')"],"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0],\n","        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0]])"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"1A9feXpDKRSr","colab_type":"code","colab":{}},"source":["N_LETTERS = 26\n","\n","WORD_EMBEDDING_SIZE = 6\n","HIDDEN_DIM_LETTER_LSTM = 5\n","TOTAL_EMBEDDING_SIZE = WORD_EMBEDDING_SIZE + HIDDEN_DIM_LETTER_LSTM\n","\n","HIDDEN_DIM_FULL_LSTM = 6\n","\n","VOCAB_SIZE = len(word_id)\n","\n","TAGSET_SIZE = len(tag_id)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QkMXsMRupHzc","colab_type":"code","colab":{}},"source":["def init_hidden(dim):\n","        return (torch.zeros(1, 1, dim),\n","                torch.zeros(1, 1, dim))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xzubr5h7nl_1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":302},"outputId":"8f42a889-b99b-4248-ea38-77ba4946dc27","executionInfo":{"status":"ok","timestamp":1588328478120,"user_tz":-60,"elapsed":657,"user":{"displayName":"Skander Ben Mansour","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl7domjq2nPtW4SYKR5BEYuj4qkl2Ljge9t-vlBiU=s64","userId":"11506237701449758837"}}},"source":["## Try the simple LSTM by hand (no letter embedding)\n","\n","\n","nn_word_embeddings = nn.Embedding(VOCAB_SIZE, WORD_EMBEDDING_SIZE)\n","lstm_full = nn.LSTM(WORD_EMBEDDING_SIZE, HIDDEN_DIM_FULL_LSTM)  \n","hidden_full = init_hidden(HIDDEN_DIM_FULL_LSTM)\n","\n","sentence = 'the dog ate the apple'.split()\n","n_words = len(sentence)\n","word_ids = prepare_sequence(sentence,word_id)\n","print(word_ids)\n","word_embeddings = nn_word_embeddings(word_ids)\n","print(word_embeddings)\n","\n","\n","for word_embedding in word_embeddings:\n","    print(word_embedding.dtype)\n","    # Step through the sequence one element at a time.\n","    # after each step, hidden contains the hidden state.\n","    out, hidden_full = lstm_full(word_embedding.view(1, 1, -1), hidden_full)\n","\n","print(out)\n","# out is our final embedding\n","print(hidden_full)"],"execution_count":61,"outputs":[{"output_type":"stream","text":["tensor([0, 1, 2, 0, 3])\n","tensor([[ 1.1187, -1.0463, -0.2531,  0.1002, -0.1832,  2.3387],\n","        [-1.8146, -0.5631, -0.5191, -1.2617,  0.4364,  1.1755],\n","        [-1.6904, -0.4445, -0.9425, -0.9120, -0.0203, -0.0618],\n","        [ 1.1187, -1.0463, -0.2531,  0.1002, -0.1832,  2.3387],\n","        [-1.4467,  0.7909,  0.7926, -0.6782, -0.4936, -2.6210]],\n","       grad_fn=<EmbeddingBackward>)\n","torch.float32\n","torch.float32\n","torch.float32\n","torch.float32\n","torch.float32\n","tensor([[[ 0.1822,  0.0649, -0.0623, -0.0042,  0.0387, -0.0080]]],\n","       grad_fn=<StackBackward>)\n","(tensor([[[ 0.1822,  0.0649, -0.0623, -0.0042,  0.0387, -0.0080]]],\n","       grad_fn=<StackBackward>), tensor([[[ 0.2880,  0.3061, -0.0800, -0.0076,  0.1170, -0.0522]]],\n","       grad_fn=<StackBackward>))\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XlW6cC9wqqzg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":302},"outputId":"3cf7ba6e-c89b-42a3-8faa-8fdfcd2ff088","executionInfo":{"status":"ok","timestamp":1588327708889,"user_tz":-60,"elapsed":557,"user":{"displayName":"Skander Ben Mansour","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl7domjq2nPtW4SYKR5BEYuj4qkl2Ljge9t-vlBiU=s64","userId":"11506237701449758837"}}},"source":["## Try the letter LSTM by hand\n","\n","\n","\n","lstm_letters = nn.LSTM(N_LETTERS, HIDDEN_DIM_LETTER_LSTM)  \n","hidden_letters = init_hidden(HIDDEN_DIM_LETTER_LSTM)\n","\n","word = 'orange'\n","n_letters = len(word)\n","letter_ids = get_word_vector(word).type(torch.FloatTensor)\n","print(letter_ids)\n","\n","\n","for letter in letter_ids:\n","    # Step through the sequence one element at a time.\n","    # after each step, hidden contains the hidden state.\n","    letter_out, hidden_letters = lstm_letters(letter.view(1, 1, -1), hidden_letters)\n","\n","print(letter_out)\n","# letter_out is our letter level embedding for the word\n","print(hidden_letters)"],"execution_count":57,"outputs":[{"output_type":"stream","text":["tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n","         0., 0., 0., 0., 0., 0., 0., 0.],\n","        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","         0., 0., 0., 0., 0., 0., 0., 0.]])\n","tensor([[[ 0.1705, -0.1399,  0.1995,  0.2432, -0.0396]]],\n","       grad_fn=<StackBackward>)\n","(tensor([[[ 0.1705, -0.1399,  0.1995,  0.2432, -0.0396]]],\n","       grad_fn=<StackBackward>), tensor([[[ 0.4784, -0.2192,  0.4624,  0.4582, -0.0817]]],\n","       grad_fn=<StackBackward>))\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"79WIt48ytHow","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":302},"outputId":"718709ed-3d1c-463b-a349-c100213971dd","executionInfo":{"status":"ok","timestamp":1588328496858,"user_tz":-60,"elapsed":543,"user":{"displayName":"Skander Ben Mansour","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl7domjq2nPtW4SYKR5BEYuj4qkl2Ljge9t-vlBiU=s64","userId":"11506237701449758837"}}},"source":["## Putting it all together\n","\n","\n","\n","lstm_letters = nn.LSTM(N_LETTERS, HIDDEN_DIM_LETTER_LSTM)  \n","hidden_letters = init_hidden(HIDDEN_DIM_LETTER_LSTM)\n","\n","nn_word_embeddings = nn.Embedding(VOCAB_SIZE, WORD_EMBEDDING_SIZE)\n","# the full LSTM now takes a larger input : concatenating the original word embedding + letter level embedding (output of first LSTM)\n","lstm_full = nn.LSTM(WORD_EMBEDDING_SIZE+HIDDEN_DIM_LETTER_LSTM, HIDDEN_DIM_FULL_LSTM)  \n","hidden_full = init_hidden(HIDDEN_DIM_FULL_LSTM)\n","\n","\n","\n","\n","sentence = 'the dog ate the apple'.split()\n","n_words = len(sentence)\n","word_ids = prepare_sequence(sentence,word_id)\n","print(word_ids)\n","word_embeddings = nn_word_embeddings(word_ids)\n","print(word_embeddings)\n","\n","\n","for pos in range(n_words):\n","    word_embedding = word_embeddings[pos]\n","    print(word_embedding.dtype)\n","    \n","    word = sentence[pos]\n","    n_letters = len(word)\n","    letter_ids = get_word_vector(word).type(torch.FloatTensor)\n","\n","    for letter in letter_ids:\n","        letter_out, hidden_letters = lstm_letters(letter.view(1, 1, -1), hidden_letters)\n","\n","    # we prepare to concatenate the word embedding and letter level embedding\n","    letter_out = letter_out.view(-1,1)\n","    word_embedding = word_embedding.view(-1,1)\n","    full_lstm_input = torch.cat([word_embedding,letter_out])\n","\n","    # and can then feed this into the full LSTM\n","    out, hidden_full = lstm_full(full_lstm_input.view(1, 1, -1), hidden_full)\n","\n","\n","print(out)\n","# out is our final embedding\n","print(hidden_full)\n","\n"],"execution_count":62,"outputs":[{"output_type":"stream","text":["tensor([0, 1, 2, 0, 3])\n","tensor([[ 1.3718,  0.1913,  0.6198, -0.1384,  1.3611,  1.0847],\n","        [ 1.4003,  0.5050,  1.2077, -2.8796,  0.4563,  0.6948],\n","        [ 0.6902,  0.2315, -0.3435, -0.8470, -0.0889,  2.3110],\n","        [ 1.3718,  0.1913,  0.6198, -0.1384,  1.3611,  1.0847],\n","        [ 0.2111,  0.0057, -1.0064, -0.3516, -0.6723,  0.8524]],\n","       grad_fn=<EmbeddingBackward>)\n","torch.float32\n","torch.float32\n","torch.float32\n","torch.float32\n","torch.float32\n","tensor([[[ 0.4261,  0.1717, -0.1574,  0.0562, -0.0177, -0.0064]]],\n","       grad_fn=<StackBackward>)\n","(tensor([[[ 0.4261,  0.1717, -0.1574,  0.0562, -0.0177, -0.0064]]],\n","       grad_fn=<StackBackward>), tensor([[[ 0.8067,  0.3278, -0.2929,  0.1296, -0.0433, -0.0210]]],\n","       grad_fn=<StackBackward>))\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yMQXJhfFDRAJ","colab_type":"text"},"source":["Create the model:\n","\n"]},{"cell_type":"code","metadata":{"id":"ZxLqCM84DRAL","colab_type":"code","colab":{}},"source":["class CharLevelLSTM(nn.Module):\n","\n","    def __init__(self):\n","        super(CharLevelLSTM, self).__init__()\n","\n","        self.lstm_letters = nn.LSTM(N_LETTERS, HIDDEN_DIM_LETTER_LSTM)  \n","        self.hidden_letters = init_hidden(HIDDEN_DIM_LETTER_LSTM)\n","\n","        self.nn_word_embeddings = nn.Embedding(VOCAB_SIZE, WORD_EMBEDDING_SIZE)\n","        # the full LSTM now takes a larger input : concatenating the original word embedding + letter level embedding (output of first LSTM)\n","        self.lstm_full = nn.LSTM(WORD_EMBEDDING_SIZE+HIDDEN_DIM_LETTER_LSTM, HIDDEN_DIM_FULL_LSTM)  \n","        self.hidden_full = init_hidden(HIDDEN_DIM_FULL_LSTM)\n","\n","\n","        # The linear layer that maps from hidden state space to tag space\n","        self.hidden2tag = nn.Linear(HIDDEN_DIM_FULL_LSTM, TAGSET_SIZE)\n","        \n","\n","    \n","\n","    def forward(self, sentence):\n","        # note the input is now actual words, not word_ids\n","\n","        n_words = len(sentence)\n","\n","        word_ids = prepare_sequence(sentence,word_id)\n","        # print(word_ids)\n","        word_embeddings = self.nn_word_embeddings(word_ids)\n","        # print(word_embeddings)\n","\n","\n","        for pos in range(n_words):\n","            word_embedding = word_embeddings[pos]\n","            # print(word_embedding.dtype)\n","            \n","            word = sentence[pos]\n","            n_letters = len(word)\n","            letter_ids = get_word_vector(word).type(torch.FloatTensor)\n","\n","            for letter in letter_ids:\n","                letter_out, self.hidden_letters = self.lstm_letters(letter.view(1, 1, -1), self.hidden_letters)\n","\n","            # we prepare to concatenate the word embedding and letter level embedding\n","            letter_out = letter_out.view(-1,1)\n","            word_embedding = word_embedding.view(-1,1)\n","            full_lstm_input = torch.cat([word_embedding,letter_out])\n","\n","            # and can then feed this into the full LSTM\n","            out, self.hidden_full = self.lstm_full(full_lstm_input.view(1, 1, -1), self.hidden_full)\n","\n","\n","        # print(out)\n","        # out is our final embedding\n","        # print(hidden_full)\n","\n","        tag_space = self.hidden2tag(out.view(n_words, -1))\n","        tag_scores = F.log_softmax(tag_space, dim=1)\n","        return tag_scores"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B_xLRqENxUmX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ce228119-6507-4c2e-c94c-42648824423a","executionInfo":{"status":"ok","timestamp":1588328751367,"user_tz":-60,"elapsed":542,"user":{"displayName":"Skander Ben Mansour","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl7domjq2nPtW4SYKR5BEYuj4qkl2Ljge9t-vlBiU=s64","userId":"11506237701449758837"}}},"source":["training_data[0][0]"],"execution_count":65,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['the', 'dog', 'ate', 'the', 'apple']"]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"markdown","metadata":{"id":"xQL824RODRAT","colab_type":"text"},"source":["Train the model:\n","\n"]},{"cell_type":"code","metadata":{"id":"UJD0omS4DRAV","colab_type":"code","outputId":"7505a358-aeef-46d6-f72f-a41586029b9f","executionInfo":{"status":"error","timestamp":1588328788770,"user_tz":-60,"elapsed":510,"user":{"displayName":"Skander Ben Mansour","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggl7domjq2nPtW4SYKR5BEYuj4qkl2Ljge9t-vlBiU=s64","userId":"11506237701449758837"}},"colab":{"base_uri":"https://localhost:8080/","height":354}},"source":["model = CharLevelLSTM()\n","loss_function = nn.NLLLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.1)\n","\n","# See what the scores are before training\n","# Note that element i,j of the output is the score for tag j for word i.\n","# Here we don't need to train, so the code is wrapped in torch.no_grad()\n","with torch.no_grad():\n","    tag_scores = model(training_data[0][0])\n","    print(tag_scores)\n"],"execution_count":67,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-67-22f87bf69678>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Here we don't need to train, so the code is wrapped in torch.no_grad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtag_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-63-86ce04de8350>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# print(hidden_full)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mtag_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mtag_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtag_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: shape '[5, -1]' is invalid for input of size 6"]}]},{"cell_type":"code","metadata":{"id":"n61pmVVaTmmk","colab_type":"code","colab":{}},"source":["\n","for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n","    for sentence, tags in training_data:\n","        # Step 1. Remember that Pytorch accumulates gradients.\n","        # We need to clear them out before each instance\n","        model.zero_grad()\n","\n","        # Also, we need to clear out the hidden state of the LSTM,\n","        # detaching it from its history on the last instance.\n","        model.hidden = model.init_hidden()\n","\n","        # Step 2. Get our inputs ready for the network, that is, turn them into\n","        # Tensors of word indices.\n","        sentence_in = prepare_sequence(sentence, word_to_ix)\n","        targets = prepare_sequence(tags, tag_to_ix)\n","\n","        # Step 3. Run our forward pass.\n","        tag_scores = model(sentence_in)\n","\n","        # Step 4. Compute the loss, gradients, and update the parameters by\n","        #  calling optimizer.step()\n","        loss = loss_function(tag_scores, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","# See what the scores are after training\n","with torch.no_grad():\n","    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n","    tag_scores = model(inputs)\n","\n","    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n","    # for word i. The predicted tag is the maximum scoring tag.\n","    # Here, we can see the predicted sequence below is 0 1 2 0 1\n","    # since 0 is index of the maximum value of row 1,\n","    # 1 is the index of maximum value of row 2, etc.\n","    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n","    print(tag_scores)"],"execution_count":0,"outputs":[]}]}